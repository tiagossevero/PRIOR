{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üìä Gerador de Data Schemas - GECOB\n",
    "\n",
    "Este notebook executa o script para gerar automaticamente os data schemas das tabelas do GECOB.\n",
    "\n",
    "**Tabelas documentadas:**\n",
    "1. `gecob.prior_master_consolidado`\n",
    "2. `gecob.prior_score_priorizacao`\n",
    "3. `gecob.prior_score_componentes`\n",
    "4. `gecob.prior_clusters_empresas`\n",
    "5. `gecob.prior_outliers_identificados`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## üîß 1. Setup do Ambiente\n",
    "\n",
    "Configura√ß√£o do Spark e imports necess√°rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar path das libs internas (se necess√°rio)\n",
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "# Imports PySpark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "# Imports libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "print(\"‚úÖ Imports realizados com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spark-session",
   "metadata": {},
   "source": [
    "## üöÄ 2. Inicializar SparkSession\n",
    "\n",
    "Criar ou obter a sess√£o Spark ativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-spark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar perfil e app name\n",
    "profile = 'prod'  # ou 'dev', 'hom', conforme seu ambiente\n",
    "app_name = \"tsevero_gerar_data_schemas\"\n",
    "\n",
    "# Criar SparkSession\n",
    "spark_builder = (utils.DBASparkAppSession\n",
    "                 .builder\n",
    "                 .setAppName(app_name)\n",
    "                 .usingProcessProfile(profile)\n",
    "                 .enableHiveSupport()\n",
    "                 .enableHudiSupport())\n",
    "\n",
    "# Build session\n",
    "spark_session = spark_builder.build()\n",
    "spark = spark_session.spark\n",
    "\n",
    "print(\"‚úÖ SparkSession criada com sucesso\")\n",
    "print(f\"   App Name: {app_name}\")\n",
    "print(f\"   Profile: {profile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify",
   "metadata": {},
   "source": [
    "## ‚úÖ 3. Verificar Acesso √†s Tabelas\n",
    "\n",
    "Teste r√°pido para confirmar acesso √†s tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar acesso √†s tabelas\n",
    "tabelas_teste = [\n",
    "    'prior_master_consolidado',\n",
    "    'prior_score_priorizacao',\n",
    "    'prior_score_componentes',\n",
    "    'prior_clusters_empresas',\n",
    "    'prior_outliers_identificados'\n",
    "]\n",
    "\n",
    "print(\"üîç Verificando acesso √†s tabelas...\\n\")\n",
    "\n",
    "for tabela in tabelas_teste:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as total FROM gecob.{tabela}\").collect()[0]['total']\n",
    "        print(f\"‚úÖ {tabela:35s} - {count:,} registros\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {tabela:35s} - ERRO: {str(e)[:50]}\")\n",
    "\n",
    "print(\"\\n‚úÖ Verifica√ß√£o conclu√≠da\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "execute",
   "metadata": {},
   "source": [
    "## üéØ 4. Executar Gerador de Schemas\n",
    "\n",
    "Executa o script que gera todos os data schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-generator",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importar o script\n",
    "import gerar_data_schemas\n",
    "\n",
    "# Executar\n",
    "gerar_data_schemas.main(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "## üìÅ 5. Verificar Arquivos Gerados\n",
    "\n",
    "Listar os arquivos criados no diret√≥rio `data_schemas/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "output_dir = 'data_schemas'\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    arquivos = sorted(glob.glob(f\"{output_dir}/*\"))\n",
    "    \n",
    "    print(f\"üìÅ Arquivos gerados em '{output_dir}/':\")\n",
    "    print(f\"   Total: {len(arquivos)} arquivos\\n\")\n",
    "    \n",
    "    # Agrupar por tipo\n",
    "    tipos = {}\n",
    "    for arquivo in arquivos:\n",
    "        nome = os.path.basename(arquivo)\n",
    "        ext = os.path.splitext(nome)[1]\n",
    "        if ext not in tipos:\n",
    "            tipos[ext] = []\n",
    "        tipos[ext].append(nome)\n",
    "    \n",
    "    for ext, files in sorted(tipos.items()):\n",
    "        print(f\"\\n{ext.upper() if ext else 'Outros':}:\")\n",
    "        for f in files:\n",
    "            size = os.path.getsize(os.path.join(output_dir, f))\n",
    "            size_kb = size / 1024\n",
    "            print(f\"   - {f:50s} ({size_kb:>8.1f} KB)\")\n",
    "else:\n",
    "    print(f\"‚ùå Diret√≥rio '{output_dir}/' n√£o encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preview",
   "metadata": {},
   "source": [
    "## üëÅÔ∏è 6. Preview dos Resultados\n",
    "\n",
    "Visualizar alguns exemplos dos schemas gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preview-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exemplo: ver describe da tabela master\n",
    "print(\"üìä DESCRIBE FORMATTED - prior_master_consolidado (primeiras 30 linhas)\\n\")\n",
    "\n",
    "df = pd.read_csv('data_schemas/prior_master_consolidado_describe_formatted.csv')\n",
    "display(df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preview-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: ver sample de dados\n",
    "print(\"üìã SAMPLE DATA - prior_score_priorizacao\\n\")\n",
    "\n",
    "df_sample = pd.read_csv('data_schemas/prior_score_priorizacao_sample_10.csv')\n",
    "display(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "json-consolidated",
   "metadata": {},
   "source": [
    "## üìÑ 7. Ver JSON Consolidado\n",
    "\n",
    "Carregar e explorar o JSON consolidado com todos os schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-json",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Carregar JSON consolidado\n",
    "with open('data_schemas/data_schemas_completo.json', 'r', encoding='utf-8') as f:\n",
    "    schemas = json.load(f)\n",
    "\n",
    "print(\"üìä Resumo do JSON Consolidado:\\n\")\n",
    "\n",
    "for schema in schemas:\n",
    "    tabela = schema['tabela']\n",
    "    row_count = schema.get('row_count', 'N/A')\n",
    "    \n",
    "    if 'error' not in schema:\n",
    "        print(f\"‚úÖ {tabela:35s} - {row_count:>10,} registros\")\n",
    "    else:\n",
    "        print(f\"‚ùå {tabela:35s} - ERRO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## üßπ 8. Cleanup (Opcional)\n",
    "\n",
    "Encerrar sess√£o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stop-spark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar para encerrar a sess√£o Spark\n",
    "# spark.stop()\n",
    "# print(\"‚úÖ SparkSession encerrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Processo Conclu√≠do!\n",
    "\n",
    "Os data schemas foram gerados com sucesso. Voc√™ pode:\n",
    "\n",
    "1. **Consultar o README**: `data_schemas/README.md`\n",
    "2. **Ver schemas em Markdown**: `data_schemas/*_describe_formatted.md`\n",
    "3. **Importar CSVs**: `data_schemas/*_describe_formatted.csv`\n",
    "4. **Usar JSON**: `data_schemas/data_schemas_completo.json`\n",
    "\n",
    "---\n",
    "\n",
    "**GECOB - Sistema de Prioriza√ß√£o de Cobran√ßa**  \n",
    "Receita Estadual de Santa Catarina"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
