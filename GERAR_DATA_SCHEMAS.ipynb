{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerador Automático de Data Schemas - GECOB\n\nProcessa 5 tabelas e gera schemas completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURAÇÕES\nDATABASE = 'gecob'\nTABELAS = ['prior_master_consolidado', 'prior_score_priorizacao', 'prior_score_componentes', 'prior_clusters_empresas', 'prior_outliers_identificados']\nOUTPUT_DIR = 'data_schemas'\nSAMPLE_LIMIT = 10\nprint('Config OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\nsys.path.append('/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags')\n\nimport os\nimport json\nimport builtins\nfrom datetime import datetime\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\ntry:\n    from utils import spark_utils_session as utils\n    print('Utils importado')\nexcept:\n    print('Sem utils - use SparkSession manual')\n\nprint('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK SESSION\nprofile = 'prod'\napp_name = 'tsevero_gerar_data_schemas'\n\nspark_builder = (utils.DBASparkAppSession.builder.setAppName(app_name).usingProcessProfile(profile).enableHiveSupport().enableHudiSupport())\nspark_session = spark_builder.build()\nspark = spark_session.spark\n\nprint(f'Spark OK: {app_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAR ACESSO\nprint('Verificando tabelas...')\nfor t in TABELAS:\n    try:\n        cnt = spark.sql(f'SELECT COUNT(*) as c FROM {DATABASE}.{t}').collect()[0]['c']\n        print(f'  OK {t:40s} {cnt:,} registros')\n    except Exception as e:\n        print(f'  ERRO {t}: {str(e)[:60]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÕES AUXILIARES\n\ndef criar_dir():\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR)\n        print(f'Dir criado: {OUTPUT_DIR}')\n\ndef salvar_csv(df, nome):\n    path = os.path.join(OUTPUT_DIR, nome)\n    df.to_csv(path, index=False, encoding='utf-8-sig')\n    return path\n\ndef salvar_json(df, nome):\n    path = os.path.join(OUTPUT_DIR, nome)\n    df.to_json(path, orient='records', indent=2, force_ascii=False)\n    return path\n\ndef salvar_md(df, nome, titulo):\n    path = os.path.join(OUTPUT_DIR, nome)\n    with open(path, 'w') as f:\n        f.write(f'# {titulo}\\n\\n')\n        f.write(df.to_markdown(index=False))\n    return path\n\nprint('Funcoes OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO PROCESSAR TABELA\n\ndef processar_tabela(tabela):\n    print(f'\\n{\"=\"*60}')\n    print(f'Processando: {DATABASE}.{tabela}')\n    print('='*60)\n    \n    resultado = {'database': DATABASE, 'tabela': tabela, 'timestamp': datetime.now().isoformat()}\n    \n    try:\n        # DESCRIBE\n        print('  1. DESCRIBE FORMATTED...')\n        df_desc = spark.sql(f'DESCRIBE FORMATTED {DATABASE}.{tabela}').toPandas()\n        salvar_csv(df_desc, f'{tabela}_describe_formatted.csv')\n        salvar_json(df_desc, f'{tabela}_describe_formatted.json')\n        salvar_md(df_desc, f'{tabela}_describe_formatted.md', f'DESCRIBE {DATABASE}.{tabela}')\n        print(f'     OK - {len(df_desc)} linhas')\n        \n        # SAMPLE\n        print(f'  2. SELECT * LIMIT {SAMPLE_LIMIT}...')\n        df_sample = spark.sql(f'SELECT * FROM {DATABASE}.{tabela} LIMIT {SAMPLE_LIMIT}').toPandas()\n        salvar_csv(df_sample, f'{tabela}_sample_{SAMPLE_LIMIT}.csv')\n        salvar_json(df_sample, f'{tabela}_sample_{SAMPLE_LIMIT}.json')\n        salvar_md(df_sample, f'{tabela}_sample_{SAMPLE_LIMIT}.md', f'SAMPLE {DATABASE}.{tabela}')\n        print(f'     OK - {len(df_sample)} x {len(df_sample.columns)} colunas')\n        \n        # COUNT\n        print('  3. COUNT(*) ...')\n        cnt = spark.sql(f'SELECT COUNT(*) as c FROM {DATABASE}.{tabela}').collect()[0]['c']\n        resultado['row_count'] = int(cnt)\n        print(f'     OK - {cnt:,} registros')\n        \n        return resultado\n    except Exception as e:\n        print(f'  ERRO: {str(e)}')\n        resultado['error'] = str(e)\n        return resultado\n\nprint('Funcao processar OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTAR GERADOR\nprint('='*60)\nprint('EXECUTANDO GERADOR')\nprint('='*60)\n\ncriar_dir()\nresultados = []\n\nfor i, tabela in enumerate(TABELAS, 1):\n    print(f'\\n[{i}/{len(TABELAS)}] {tabela}')\n    resultado = processar_tabela(tabela)\n    resultados.append(resultado)\n\n# JSON consolidado\nwith open(f'{OUTPUT_DIR}/data_schemas_completo.json', 'w') as f:\n    json.dump(resultados, f, indent=2, ensure_ascii=False)\n\n# README\nwith open(f'{OUTPUT_DIR}/README.md', 'w') as f:\n    f.write('# Data Schemas - GECOB\\n\\n')\n    f.write(f'Database: {DATABASE}\\n\\n')\n    for r in resultados:\n        if 'error' not in r:\n            f.write(f\"## {r['tabela']}\\n\")\n            f.write(f\"- Registros: {r['row_count']:,}\\n\\n\")\n\nsucessos = [r for r in resultados if 'error' not in r]\nerros = [r for r in resultados if 'error' in r]\n\nprint(f'\\nCONCLUÍDO!')\nprint(f'Sucessos: {len(sucessos)}')\nprint(f'Erros: {len(erros)}')\nprint(f'Output: {OUTPUT_DIR}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTAR ARQUIVOS GERADOS\nimport glob\n\narquivos = sorted(glob.glob(f'{OUTPUT_DIR}/*'))\nprint(f'Total: {len(arquivos)} arquivos\\n')\n\nfor arq in arquivos:\n    nome = os.path.basename(arq)\n    tamanho = os.path.getsize(arq) / 1024\n    print(f'  {nome:50s} {tamanho:8.1f} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIEW DOS RESULTADOS\nimport pandas as pd\n\n# Ver describe da primeira tabela\ntabela = TABELAS[0]\ndf = pd.read_csv(f'{OUTPUT_DIR}/{tabela}_describe_formatted.csv')\nprint(f'DESCRIBE {tabela}:\\n')\ndisplay(df.head(20))\n\n# Ver sample\ndf_sample = pd.read_csv(f'{OUTPUT_DIR}/{tabela}_sample_{SAMPLE_LIMIT}.csv')\nprint(f'\\nSAMPLE {tabela}:\\n')\ndisplay(df_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}