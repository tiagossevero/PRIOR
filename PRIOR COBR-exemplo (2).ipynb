{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aea681-2477-45f5-9ffe-1cb8f6cb1a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "#Import libs python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "#Import libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf8a1f-48e4-4c57-a162-c839c8e3a940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Generates DBASparkAppSession.\"\"\"\n",
    "    \n",
    "    app_name = \"tsevero_cobranca\"\n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00777b24-69dc-4a6e-ba6f-4e11cd534937",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkSession.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80f43b-a7df-4774-a54c-81093d6429a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d8bf8-a229-46d4-8c06-850c68566574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas para an√°lise e visualiza√ß√£o\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from datetime import date, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Acesso ao SparkSession\n",
    "spark = session.sparkSession\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SISTEMA DE PRIORIZA√á√ÉO DE COBRAN√áA - GECOB\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSess√£o Spark: {spark.sparkContext.appName}\")\n",
    "print(f\"Vers√£o Spark: {spark.version}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a8f07-0ff7-4bf2-9788-83b525e7b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VALIDA√á√ÉO DA ESTRUTURA - SQL V1.4 (CORRIGIDA)\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"VALIDA√á√ÉO DA ESTRUTURA DAS TABELAS (SQL V1.4)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Verificar colunas da master_consolidado\n",
    "print(\"\\n1. Colunas da prior_master_consolidado:\")\n",
    "try:\n",
    "    colunas_master = spark.sql(\"DESCRIBE gecob.prior_master_consolidado\").toPandas()\n",
    "    print(f\"Total: {len(colunas_master)} colunas\")\n",
    "    print(\"\\nPrimeiras 20 colunas:\")\n",
    "    for idx, col in enumerate(colunas_master['col_name'].head(20), 1):\n",
    "        print(f\"  {idx:2d}. {col}\")\n",
    "    \n",
    "    # Verificar se h√° coluna tipo_debito\n",
    "    if 'tipo_debito' in colunas_master['col_name'].values:\n",
    "        print(\"\\n‚úÖ Coluna 'tipo_debito' encontrada\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå ATEN√á√ÉO: Coluna 'tipo_debito' N√ÉO encontrada!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO ao descrever tabela: {str(e)[:200]}\")\n",
    "\n",
    "# 2. Verificar m√©tricas temporais de contato\n",
    "print(\"\\n2. Verificando colunas importantes:\")\n",
    "colunas_importantes = [\n",
    "    'qtd_contatos_ultimos_30_dias',\n",
    "    'qtd_contatos_ultimos_90_dias',\n",
    "    'inscricao_estadual',\n",
    "    'tipo_debito',\n",
    "    'valor_total_devido'\n",
    "]\n",
    "\n",
    "try:\n",
    "    for col in colunas_importantes:\n",
    "        if col in colunas_master['col_name'].values:\n",
    "            print(f\"  ‚úÖ {col}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {col} - N√ÉO ENCONTRADA\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è  Erro na verifica√ß√£o: {str(e)[:100]}\")\n",
    "\n",
    "# 3. Verificar se score est√° em master ou apenas em score_priorizacao\n",
    "print(\"\\n3. Verificando localiza√ß√£o do score:\")\n",
    "try:\n",
    "    if 'score_final_priorizacao' in colunas_master['col_name'].values:\n",
    "        print(\"  ‚ÑπÔ∏è  Score encontrado em prior_master_consolidado\")\n",
    "    else:\n",
    "        print(\"  ‚ÑπÔ∏è  Score N√ÉO est√° em master (esperado - deve estar em prior_score_priorizacao)\")\n",
    "        \n",
    "        # Verificar tabela de score\n",
    "        colunas_score = spark.sql(\"DESCRIBE gecob.prior_score_priorizacao\").toPandas()\n",
    "        if 'score_final_priorizacao' in colunas_score['col_name'].values:\n",
    "            print(\"  ‚úÖ Score encontrado em prior_score_priorizacao\")\n",
    "        else:\n",
    "            print(\"  ‚ùå Score N√ÉO encontrado em prior_score_priorizacao\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è  Erro: {str(e)[:100]}\")\n",
    "\n",
    "# 4. Teste simples de contagem (SEM JOIN complexo)\n",
    "print(\"\\n4. Teste b√°sico de contagem:\")\n",
    "try:\n",
    "    # Contagem simples da master\n",
    "    count_master = spark.sql(\"SELECT COUNT(*) as cnt FROM gecob.prior_master_consolidado\").collect()[0]['cnt']\n",
    "    print(f\"  ‚úÖ Registros em prior_master_consolidado: {count_master:,}\")\n",
    "    \n",
    "    # Contagem da tabela de score\n",
    "    count_score = spark.sql(\"SELECT COUNT(*) as cnt FROM gecob.prior_score_priorizacao\").collect()[0]['cnt']\n",
    "    print(f\"  ‚úÖ Registros em prior_score_priorizacao: {count_score:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå ERRO: {str(e)[:200]}\")\n",
    "\n",
    "# 5. Testar JOIN simples (apenas contagem)\n",
    "print(\"\\n5. Testando JOIN master x score (vers√£o simplificada):\")\n",
    "try:\n",
    "    test_join = spark.sql(\"\"\"\n",
    "        SELECT COUNT(*) as total\n",
    "        FROM gecob.prior_master_consolidado mc\n",
    "        INNER JOIN gecob.prior_score_priorizacao sp \n",
    "            ON mc.inscricao_estadual = sp.inscricao_estadual\n",
    "            AND mc.tipo_debito = sp.tipo_debito\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    \n",
    "    # For√ßar execu√ß√£o\n",
    "    result = test_join.collect()\n",
    "    print(f\"  ‚úÖ JOIN funcionando! Total: {result[0]['total']:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if \"stopped SparkContext\" in error_msg:\n",
    "        print(f\"  ‚ùå SparkContext parado - REINICIE A C√âLULA 2 (Sess√£o Spark)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå ERRO no JOIN: {error_msg[:200]}\")\n",
    "\n",
    "# 6. Verificar duplicatas (vers√£o simplificada)\n",
    "print(\"\\n6. Verificando duplicatas (amostra):\")\n",
    "try:\n",
    "    dups_sample = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            inscricao_estadual,\n",
    "            tipo_debito,\n",
    "            COUNT(*) as qtd\n",
    "        FROM gecob.prior_master_consolidado\n",
    "        GROUP BY inscricao_estadual, tipo_debito\n",
    "        HAVING COUNT(*) > 1\n",
    "        LIMIT 5\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    if len(dups_sample) > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  ATEN√á√ÉO: Encontradas {len(dups_sample)} chaves duplicadas (amostra)\")\n",
    "        for row in dups_sample[:3]:\n",
    "            print(f\"     IE: {row['inscricao_estadual']} | Tipo: {row['tipo_debito']} | Qtd: {row['qtd']}\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Nenhuma duplicata encontrada (chave: IE + tipo_debito)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    if \"stopped\" in error_msg.lower():\n",
    "        print(f\"  ‚ùå SparkContext parado - REINICIE o notebook\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå ERRO: {error_msg[:200]}\")\n",
    "\n",
    "# 7. Estat√≠sticas b√°sicas r√°pidas\n",
    "print(\"\\n7. Estat√≠sticas b√°sicas:\")\n",
    "try:\n",
    "    stats = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT inscricao_estadual) as empresas,\n",
    "            COUNT(*) as registros,\n",
    "            CAST(SUM(valor_total_devido) AS DOUBLE) as valor_total\n",
    "        FROM gecob.prior_master_consolidado\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"  Empresas √∫nicas: {stats['empresas']:,}\")\n",
    "    print(f\"  Total registros: {stats['registros']:,}\")\n",
    "    print(f\"  Valor total: R$ {stats['valor_total']/1e9:.2f} bilh√µes\")\n",
    "    print(f\"  M√©dia de d√©bitos/empresa: {stats['registros']/stats['empresas']:.1f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå ERRO: {str(e)[:200]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ VALIDA√á√ÉO CONCLU√çDA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüí° PR√ìXIMOS PASSOS:\")\n",
    "print(\"   1. Se houver erro de SparkContext parado, REINICIE a c√©lula 2\")\n",
    "print(\"   2. Se o JOIN funcionou, pode continuar com as an√°lises\")\n",
    "print(\"   3. Se houver duplicatas, verifique o SQL V1.4\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb6645-d9a0-4600-8a23-8d6fd4bb330e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 3: VERIFICA√á√ÉO DE TABELAS DISPON√çVEIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICA√á√ÉO DE TABELAS DO SISTEMA GECOB\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tabelas_gecob = [\n",
    "    'gecob.prior_debitos_base',\n",
    "    'gecob.prior_conta_corrente',\n",
    "    'gecob.prior_contribuintes_cadastro',\n",
    "    'gecob.prior_historico_contatos',\n",
    "    'gecob.prior_parcelamentos',\n",
    "    'gecob.prior_score_noteiras',\n",
    "    'gecob.prior_pagamentos_historico',\n",
    "    'gecob.prior_declaracoes_dime',\n",
    "    'gecob.prior_master_consolidado',\n",
    "    'gecob.prior_features_ml',\n",
    "    'gecob.prior_benchmark_setorial',\n",
    "    'gecob.prior_score_componentes',\n",
    "    'gecob.prior_score_priorizacao'\n",
    "]\n",
    "\n",
    "print(\"\\nVerificando tabelas do sistema GECOB:\\n\")\n",
    "tabelas_disponiveis = {}\n",
    "for tabela in tabelas_gecob:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {tabela}\").collect()[0]['cnt']\n",
    "        print(f\"‚úì {tabela:50s} -> {count:,} registros\")\n",
    "        tabelas_disponiveis[tabela] = count\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {tabela:50s} -> N√ÉO ENCONTRADA\")\n",
    "        tabelas_disponiveis[tabela] = 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17742d-e289-45f6-9fcc-d2fc7b14f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 4: PANORAMA GERAL DO SISTEMA (ATUALIZADA V1.4)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1. PANORAMA GERAL DO SISTEMA DE PRIORIZA√á√ÉO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query_panorama = \"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT inscricao_estadual) AS total_empresas,\n",
    "    COUNT(*) AS total_debitos,\n",
    "    SUM(valor_total_devido) AS valor_total_cobranca,\n",
    "    AVG(valor_total_devido) AS valor_medio_debito,\n",
    "    MAX(valor_total_devido) AS maior_debito,\n",
    "    \n",
    "    -- Distribui√ß√£o por Status\n",
    "    SUM(CASE WHEN status_debito = 1 THEN 1 ELSE 0 END) AS debitos_ativos,\n",
    "    SUM(CASE WHEN status_debito = 2 THEN 1 ELSE 0 END) AS debitos_pagos,\n",
    "    \n",
    "    -- Flags de Risco\n",
    "    SUM(CASE WHEN flag_falencia = 1 THEN 1 ELSE 0 END) AS empresas_falencia,\n",
    "    SUM(CASE WHEN flag_recuperacao_judicial = 1 THEN 1 ELSE 0 END) AS empresas_recuperacao,\n",
    "    SUM(CASE WHEN flag_devedor_contumaz = 1 THEN 1 ELSE 0 END) AS devedores_contumazes,\n",
    "    \n",
    "    -- Contatos (ATUALIZADO V1.4)\n",
    "    AVG(qtd_total_contatos) AS media_contatos,\n",
    "    AVG(taxa_resposta_contatos) AS media_taxa_resposta,\n",
    "    AVG(COALESCE(qtd_contatos_ultimos_30_dias, 0)) AS media_contatos_30d,\n",
    "    AVG(COALESCE(qtd_contatos_ultimos_90_dias, 0)) AS media_contatos_90d,\n",
    "    \n",
    "    -- Pagamentos\n",
    "    AVG(qtd_periodos_pagamento_6m) AS media_periodos_pagamento,\n",
    "    AVG(valor_total_pago_6m) AS media_valor_pago_6m,\n",
    "    \n",
    "    -- Noteiras\n",
    "    AVG(score_noteiras) AS media_score_noteiras,\n",
    "    SUM(CASE WHEN classificacao_risco_noteiras IN ('ALTO', 'CRITICO') THEN 1 ELSE 0 END) AS empresas_alto_risco_noteiras\n",
    "\n",
    "FROM gecob.prior_master_consolidado\n",
    "\"\"\"\n",
    "\n",
    "df_panorama = spark.sql(query_panorama)\n",
    "panorama = df_panorama.collect()[0]\n",
    "\n",
    "print(f\"\\nM√âTRICAS PRINCIPAIS:\")\n",
    "print(f\"  Total de Empresas em Cobran√ßa: {panorama['total_empresas']:,}\")\n",
    "print(f\"  Total de D√©bitos: {panorama['total_debitos']:,}\")\n",
    "print(f\"  Valor Total em Cobran√ßa: R$ {panorama['valor_total_cobranca']:,.2f}\")\n",
    "print(f\"  Valor M√©dio por D√©bito: R$ {panorama['valor_medio_debito']:,.2f}\")\n",
    "print(f\"  Maior D√©bito: R$ {panorama['maior_debito']:,.2f}\")\n",
    "\n",
    "print(f\"\\nSTATUS DOS D√âBITOS:\")\n",
    "print(f\"  D√©bitos Ativos: {panorama['debitos_ativos']:,}\")\n",
    "print(f\"  D√©bitos Pagos: {panorama['debitos_pagos']:,}\")\n",
    "\n",
    "print(f\"\\nFLAGS DE RISCO:\")\n",
    "print(f\"  Empresas em Fal√™ncia: {panorama['empresas_falencia']:,}\")\n",
    "print(f\"  Empresas em Recupera√ß√£o Judicial: {panorama['empresas_recuperacao']:,}\")\n",
    "print(f\"  Devedores Contumazes: {panorama['devedores_contumazes']:,}\")\n",
    "\n",
    "print(f\"\\nCOMPORTAMENTO:\")\n",
    "print(f\"  M√©dia de Contatos por Empresa: {panorama['media_contatos']:.1f}\")\n",
    "print(f\"  Taxa M√©dia de Resposta: {panorama['media_taxa_resposta']:.1%}\")\n",
    "print(f\"  M√©dia Contatos √öltimos 30 dias: {panorama['media_contatos_30d']:.1f}  [NOVO V1.4]\")\n",
    "print(f\"  M√©dia Contatos √öltimos 90 dias: {panorama['media_contatos_90d']:.1f}  [NOVO V1.4]\")\n",
    "print(f\"  M√©dia Per√≠odos com Pagamento (6m): {panorama['media_periodos_pagamento']:.1f}\")\n",
    "\n",
    "print(f\"\\nNOTEIRAS:\")\n",
    "print(f\"  Score M√©dio de Noteiras: {panorama['media_score_noteiras']:.1f}\")\n",
    "print(f\"  Empresas Alto Risco (Noteiras): {panorama['empresas_alto_risco_noteiras']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc71eff-8614-4adf-aec9-a9e1b2eff84d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 5: DISTRIBUI√á√ÉO POR PRIORIDADE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. DISTRIBUI√á√ÉO POR N√çVEL DE PRIORIDADE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query_priorizacao = \"\"\"\n",
    "SELECT \n",
    "    classificacao_prioridade,\n",
    "    COUNT(*) AS quantidade,\n",
    "    COUNT(DISTINCT inscricao_estadual) AS empresas_distintas,\n",
    "    SUM(valor_total_devido) AS valor_total,\n",
    "    AVG(valor_total_devido) AS valor_medio,\n",
    "    AVG(score_final_priorizacao) AS score_medio,\n",
    "    \n",
    "    -- Componentes de Score M√©dios\n",
    "    AVG(score_valor_debito) AS media_score_valor,\n",
    "    AVG(score_capacidade_pagamento) AS media_score_capacidade,\n",
    "    AVG(score_historico_pagamento) AS media_score_historico,\n",
    "    AVG(score_responsividade) AS media_score_responsividade,\n",
    "    AVG(score_viabilidade_cobranca) AS media_score_viabilidade,\n",
    "    AVG(score_urgencia) AS media_score_urgencia,\n",
    "    AVG(score_conformidade) AS media_score_conformidade\n",
    "\n",
    "FROM gecob.prior_score_priorizacao\n",
    "GROUP BY classificacao_prioridade\n",
    "ORDER BY \n",
    "    CASE classificacao_prioridade\n",
    "        WHEN 'PRIORIDADE_MAXIMA' THEN 1\n",
    "        WHEN 'PRIORIDADE_ALTA' THEN 2\n",
    "        WHEN 'PRIORIDADE_MEDIA' THEN 3\n",
    "        WHEN 'PRIORIDADE_BAIXA' THEN 4\n",
    "    END\n",
    "\"\"\"\n",
    "\n",
    "df_priorizacao = spark.sql(query_priorizacao).toPandas()\n",
    "\n",
    "# Converter Decimal para float\n",
    "numeric_cols = df_priorizacao.select_dtypes(include=['object']).columns\n",
    "for col in numeric_cols:\n",
    "    try:\n",
    "        df_priorizacao[col] = pd.to_numeric(df_priorizacao[col], errors='ignore')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\nDISTRIBUI√á√ÉO POR PRIORIDADE:\\n\")\n",
    "for idx, row in df_priorizacao.iterrows():\n",
    "    print(f\"{row['classificacao_prioridade']}:\")\n",
    "    print(f\"  Quantidade: {int(row['quantidade']):,} d√©bitos\")\n",
    "    print(f\"  Empresas: {int(row['empresas_distintas']):,}\")\n",
    "    print(f\"  Valor Total: R$ {float(row['valor_total']):,.2f}\")\n",
    "    print(f\"  Valor M√©dio: R$ {float(row['valor_medio']):,.2f}\")\n",
    "    print(f\"  Score M√©dio: {float(row['score_medio']):.2f}\")\n",
    "    print()\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "fig.suptitle('Distribui√ß√£o por N√≠vel de Prioridade', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Pizza - Quantidade\n",
    "colors = ['#d62728', '#ff7f0e', '#ffdd70', '#2ca02c']\n",
    "axes[0, 0].pie(df_priorizacao['quantidade'], labels=df_priorizacao['classificacao_prioridade'], \n",
    "               autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[0, 0].set_title('Distribui√ß√£o de D√©bitos por Prioridade')\n",
    "\n",
    "# 2. Barras - Valor Total\n",
    "axes[0, 1].barh(df_priorizacao['classificacao_prioridade'], \n",
    "                df_priorizacao['valor_total']/1e6, color=colors)\n",
    "axes[0, 1].set_xlabel('Valor Total (Milh√µes R$)')\n",
    "axes[0, 1].set_title('Valor em Cobran√ßa por Prioridade')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# 3. Barras - Score M√©dio\n",
    "axes[1, 0].bar(range(len(df_priorizacao)), df_priorizacao['score_medio'], color=colors)\n",
    "axes[1, 0].set_xticks(range(len(df_priorizacao)))\n",
    "axes[1, 0].set_xticklabels(df_priorizacao['classificacao_prioridade'], rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Score M√©dio')\n",
    "axes[1, 0].set_title('Score M√©dio por Prioridade')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Heatmap - Componentes do Score\n",
    "score_components = df_priorizacao[['classificacao_prioridade', 'media_score_valor', \n",
    "                                     'media_score_capacidade', 'media_score_historico',\n",
    "                                     'media_score_responsividade', 'media_score_viabilidade',\n",
    "                                     'media_score_urgencia', 'media_score_conformidade']].set_index('classificacao_prioridade')\n",
    "score_components.columns = ['Valor', 'Capacidade', 'Hist√≥rico', 'Responsiv.', 'Viabilidade', 'Urg√™ncia', 'Conform.']\n",
    "sns.heatmap(score_components.T, annot=True, fmt='.1f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Score M√©dio'}, linewidths=0.5, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Componentes M√©dios do Score por Prioridade')\n",
    "axes[1, 1].set_xlabel('Prioridade')\n",
    "axes[1, 1].set_ylabel('Componente')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345aa6e-3bfa-48c8-bc86-8afa2058775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 6: TOP 50 EMPRESAS PRIORIZADAS (ATUALIZADA V1.4)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. RANKING: TOP 50 EMPRESAS PRIORIT√ÅRIAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query_top50 = \"\"\"\n",
    "SELECT \n",
    "    ROW_NUMBER() OVER (ORDER BY sp.score_final_priorizacao DESC, sp.valor_total_devido DESC) AS ranking,\n",
    "    sp.inscricao_estadual,\n",
    "    sp.tipo_debito,\n",
    "    COALESCE(mc.razao_social, 'N√ÉO INFORMADO') AS razao_social,\n",
    "    COALESCE(mc.nome_fantasia, 'N√ÉO INFORMADO') AS nome_fantasia,\n",
    "    sp.valor_total_devido,\n",
    "    sp.score_final_priorizacao,\n",
    "    sp.classificacao_prioridade,\n",
    "    COALESCE(mc.porte_por_faturamento, 'N√ÉO INFORMADO') AS porte_por_faturamento,\n",
    "    COALESCE(mc.secao_cnae, 'N/A') AS secao_cnae,\n",
    "    COALESCE(mc.descricao_secao, 'N√ÉO INFORMADO') AS descricao_secao,\n",
    "    COALESCE(mc.situacao_cadastral_desc, 'N√ÉO INFORMADO') AS situacao_cadastral_desc,\n",
    "    COALESCE(mc.codigo_municipio, 0) AS codigo_municipio,\n",
    "    COALESCE(mc.nome_municipio, 'N√ÉO INFORMADO') AS nome_municipio,\n",
    "    \n",
    "    -- Scores componentes\n",
    "    sp.score_valor_debito,\n",
    "    sp.score_capacidade_pagamento,\n",
    "    sp.score_historico_pagamento,\n",
    "    sp.score_responsividade,\n",
    "    sp.score_viabilidade_cobranca,\n",
    "    \n",
    "    -- Comportamento (ATUALIZADO V1.4)\n",
    "    COALESCE(mc.qtd_total_contatos, 0) AS qtd_total_contatos,\n",
    "    COALESCE(mc.taxa_resposta_contatos, 0) AS taxa_resposta_contatos,\n",
    "    COALESCE(mc.regularidade_pagamentos, 'N√ÉO INFORMADO') AS regularidade_pagamentos,\n",
    "    COALESCE(mc.qtd_contatos_ultimos_30_dias, 0) AS qtd_contatos_30d,\n",
    "    COALESCE(mc.qtd_contatos_ultimos_90_dias, 0) AS qtd_contatos_90d,\n",
    "    \n",
    "    -- Riscos\n",
    "    COALESCE(mc.flag_falencia, 0) AS flag_falencia,\n",
    "    COALESCE(mc.flag_recuperacao_judicial, 0) AS flag_recuperacao_judicial,\n",
    "    COALESCE(mc.flag_devedor_contumaz, 0) AS flag_devedor_contumaz,\n",
    "    COALESCE(mc.classificacao_risco_noteiras, 'SEM_INDICIO') AS classificacao_risco_noteiras,\n",
    "    COALESCE(mc.score_noteiras, 0) AS score_noteiras\n",
    "    \n",
    "FROM gecob.prior_score_priorizacao sp\n",
    "INNER JOIN gecob.prior_master_consolidado mc \n",
    "    ON sp.inscricao_estadual = mc.inscricao_estadual\n",
    "    AND sp.tipo_debito = mc.tipo_debito\n",
    "ORDER BY sp.score_final_priorizacao DESC, sp.valor_total_devido DESC\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "df_top50 = spark.sql(query_top50).toPandas()\n",
    "\n",
    "# Converter decimals e tratar nulls\n",
    "for col in df_top50.columns:\n",
    "    try:\n",
    "        df_top50[col] = pd.to_numeric(df_top50[col], errors='ignore')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Preencher nulls em strings\n",
    "string_cols = df_top50.select_dtypes(include=['object']).columns\n",
    "for col in string_cols:\n",
    "    df_top50[col] = df_top50[col].fillna('N√ÉO INFORMADO')\n",
    "\n",
    "print(f\"\\nTOP {len(df_top50)} EMPRESAS POR SCORE DE PRIORIZA√á√ÉO:\\n\")\n",
    "for idx, row in df_top50.head(20).iterrows():\n",
    "    razao = str(row['razao_social'])[:60] if row['razao_social'] else 'N√ÉO INFORMADO'\n",
    "    \n",
    "    print(f\"{int(row['ranking']):2d}. IE: {row['inscricao_estadual']} | Tipo: {row['tipo_debito']}\")\n",
    "    print(f\"    Raz√£o Social: {razao}\")\n",
    "    print(f\"    Score: {float(row['score_final_priorizacao']):.2f} | Prioridade: {row['classificacao_prioridade']}\")\n",
    "    print(f\"    Valor Devido: R$ {float(row['valor_total_devido']):,.2f}\")\n",
    "    print(f\"    Porte: {row['porte_por_faturamento']} | Setor: {str(row['descricao_secao'])[:40]}\")\n",
    "    print(f\"    Munic√≠pio: {row['nome_municipio']} | Situa√ß√£o: {row['situacao_cadastral_desc']}\")\n",
    "    print(f\"    Contatos: {int(row['qtd_total_contatos'])} total | {int(row['qtd_contatos_30d'])} (30d) | {int(row['qtd_contatos_90d'])} (90d) [V1.4]\")\n",
    "    print(f\"    Taxa Resposta: {float(row['taxa_resposta_contatos']):.1%}\")\n",
    "    \n",
    "    # Mostrar flags de risco\n",
    "    flags_risco = []\n",
    "    if int(row['flag_falencia']) == 1:\n",
    "        flags_risco.append('FAL√äNCIA')\n",
    "    if int(row['flag_recuperacao_judicial']) == 1:\n",
    "        flags_risco.append('RECUPERA√á√ÉO')\n",
    "    if int(row['flag_devedor_contumaz']) == 1:\n",
    "        flags_risco.append('CONTUMAZ')\n",
    "    if row['classificacao_risco_noteiras'] in ['ALTO', 'CRITICO']:\n",
    "        flags_risco.append(f\"NOTEIRAS:{row['classificacao_risco_noteiras']}\")\n",
    "    \n",
    "    if flags_risco:\n",
    "        print(f\"    ‚ö†Ô∏è Alertas: {', '.join(flags_risco)}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Visualiza√ß√£o - Top 20\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "fig.suptitle('An√°lise Top 20 Empresas Priorit√°rias (V1.4)', fontsize=16, fontweight='bold')\n",
    "\n",
    "top20 = df_top50.head(20).copy()\n",
    "\n",
    "# Garantir que valores num√©ricos est√£o corretos\n",
    "numeric_cols = ['score_final_priorizacao', 'valor_total_devido', 'score_valor_debito', \n",
    "                'score_capacidade_pagamento', 'score_historico_pagamento', \n",
    "                'score_responsividade', 'score_viabilidade_cobranca']\n",
    "for col in numeric_cols:\n",
    "    top20[col] = pd.to_numeric(top20[col], errors='coerce').fillna(0)\n",
    "\n",
    "# 1. Score Final\n",
    "axes[0, 0].barh(range(len(top20)), top20['score_final_priorizacao'], \n",
    "                color=['#d62728' if x >= 80 else '#ff7f0e' if x >= 60 else '#ffdd70' \n",
    "                       for x in top20['score_final_priorizacao']])\n",
    "axes[0, 0].set_yticks(range(len(top20)))\n",
    "axes[0, 0].set_yticklabels([f\"#{int(r)}\" for r in top20['ranking']])\n",
    "axes[0, 0].set_xlabel('Score Final de Prioriza√ß√£o')\n",
    "axes[0, 0].set_title('Score Final - Top 20')\n",
    "axes[0, 0].invert_yaxis()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Valor Devido\n",
    "axes[0, 1].barh(range(len(top20)), top20['valor_total_devido']/1000, color='steelblue')\n",
    "axes[0, 1].set_yticks(range(len(top20)))\n",
    "axes[0, 1].set_yticklabels([f\"#{int(r)}\" for r in top20['ranking']])\n",
    "axes[0, 1].set_xlabel('Valor Devido (Mil R$)')\n",
    "axes[0, 1].set_title('Valor do D√©bito - Top 20')\n",
    "axes[0, 1].invert_yaxis()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Heatmap componentes (Top 15)\n",
    "risk_comp = top20.head(15)[['score_valor_debito', 'score_capacidade_pagamento',\n",
    "                             'score_historico_pagamento', 'score_responsividade',\n",
    "                             'score_viabilidade_cobranca']].T\n",
    "risk_comp.columns = [f\"#{int(r)}\" for r in top20.head(15)['ranking']]\n",
    "risk_comp.index = ['Valor', 'Capacidade', 'Hist√≥rico', 'Responsiv.', 'Viabilidade']\n",
    "sns.heatmap(risk_comp, annot=True, fmt='.1f', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Score'}, linewidths=0.5, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Componentes do Score - Top 15')\n",
    "axes[1, 0].set_xlabel('Ranking')\n",
    "\n",
    "# 4. Distribui√ß√£o por Porte\n",
    "porte_dist = top20['porte_por_faturamento'].value_counts()\n",
    "colors_porte = plt.cm.Set3(np.linspace(0, 1, len(porte_dist)))\n",
    "axes[1, 1].pie(porte_dist.values, labels=porte_dist.index, autopct='%1.1f%%', \n",
    "               startangle=90, colors=colors_porte)\n",
    "axes[1, 1].set_title('Distribui√ß√£o por Porte - Top 20')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"‚úÖ An√°lise do Top 50 conclu√≠da! (V1.4)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14fd77-61a4-4097-b1c8-3af86e0fabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 7: AN√ÅLISE POR SETOR ECON√îMICO (ATUALIZADA V1.4)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. AN√ÅLISE POR SETOR ECON√îMICO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query_setores = \"\"\"\n",
    "WITH dados_unicos AS (\n",
    "    SELECT DISTINCT\n",
    "        fm.inscricao_estadual,\n",
    "        fm.secao_cnae,\n",
    "        fm.cnae_divisao,\n",
    "        fm.tipo_debito,\n",
    "        sp.valor_total_devido,\n",
    "        sp.score_final_priorizacao,\n",
    "        sp.classificacao_prioridade,\n",
    "        fm.taxa_efetividade_contatos,\n",
    "        fm.qtd_total_contatos,\n",
    "        fm.score_noteiras,\n",
    "        fm.flag_falencia\n",
    "    FROM gecob.prior_features_ml fm\n",
    "    INNER JOIN gecob.prior_score_priorizacao sp \n",
    "        ON fm.inscricao_estadual = sp.inscricao_estadual\n",
    "        AND fm.tipo_debito = sp.tipo_debito\n",
    "    WHERE fm.secao_cnae IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    secao_cnae,\n",
    "    CASE \n",
    "        WHEN secao_cnae = 'A' THEN 'AGRICULTURA, PECU√ÅRIA'\n",
    "        WHEN secao_cnae = 'B' THEN 'IND√öSTRIAS EXTRATIVAS'\n",
    "        WHEN secao_cnae = 'C' THEN 'IND√öSTRIAS DE TRANSFORMA√á√ÉO'\n",
    "        WHEN secao_cnae = 'D' THEN 'ELETRICIDADE E G√ÅS'\n",
    "        WHEN secao_cnae = 'E' THEN '√ÅGUA, ESGOTO, RES√çDUOS'\n",
    "        WHEN secao_cnae = 'F' THEN 'CONSTRU√á√ÉO'\n",
    "        WHEN secao_cnae = 'G' THEN 'COM√âRCIO'\n",
    "        WHEN secao_cnae = 'H' THEN 'TRANSPORTE, ARMAZENAGEM'\n",
    "        WHEN secao_cnae = 'I' THEN 'ALOJAMENTO E ALIMENTA√á√ÉO'\n",
    "        WHEN secao_cnae = 'J' THEN 'INFORMA√á√ÉO E COMUNICA√á√ÉO'\n",
    "        WHEN secao_cnae = 'K' THEN 'ATIVIDADES FINANCEIRAS'\n",
    "        WHEN secao_cnae = 'L' THEN 'ATIVIDADES IMOBILI√ÅRIAS'\n",
    "        WHEN secao_cnae = 'M' THEN 'ATIVIDADES PROFISSIONAIS'\n",
    "        WHEN secao_cnae = 'N' THEN 'ATIVIDADES ADMINISTRATIVAS'\n",
    "        WHEN secao_cnae = 'O' THEN 'ADMINISTRA√á√ÉO P√öBLICA'\n",
    "        WHEN secao_cnae = 'P' THEN 'EDUCA√á√ÉO'\n",
    "        WHEN secao_cnae = 'Q' THEN 'SA√öDE HUMANA'\n",
    "        WHEN secao_cnae = 'R' THEN 'ARTES, CULTURA, ESPORTE'\n",
    "        WHEN secao_cnae = 'S' THEN 'OUTRAS ATIVIDADES'\n",
    "        ELSE secao_cnae\n",
    "    END AS descricao_secao,\n",
    "    \n",
    "    COUNT(DISTINCT inscricao_estadual) AS qtd_empresas,\n",
    "    COUNT(*) AS qtd_debitos,\n",
    "    SUM(valor_total_devido) AS valor_total,\n",
    "    AVG(valor_total_devido) AS valor_medio,\n",
    "    AVG(score_final_priorizacao) AS score_medio,\n",
    "    \n",
    "    SUM(CASE WHEN classificacao_prioridade = 'PRIORIDADE_MAXIMA' THEN 1 ELSE 0 END) AS qtd_max,\n",
    "    SUM(CASE WHEN classificacao_prioridade = 'PRIORIDADE_ALTA' THEN 1 ELSE 0 END) AS qtd_alta,\n",
    "    \n",
    "    AVG(taxa_efetividade_contatos) AS media_taxa_resposta,\n",
    "    AVG(qtd_total_contatos) AS media_contatos,\n",
    "    AVG(score_noteiras) AS media_score_noteiras,\n",
    "    SUM(CASE WHEN flag_falencia = 1 THEN 1 ELSE 0 END) AS qtd_falencia\n",
    "\n",
    "FROM dados_unicos\n",
    "GROUP BY secao_cnae\n",
    "HAVING COUNT(DISTINCT inscricao_estadual) >= 100\n",
    "ORDER BY valor_total DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "df_setores = spark.sql(query_setores).toPandas()\n",
    "\n",
    "if len(df_setores) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  AVISO: Nenhum setor encontrado!\")\n",
    "else:\n",
    "    # Converter dados\n",
    "    for col in df_setores.columns:\n",
    "        if col in ['secao_cnae', 'descricao_secao']:\n",
    "            df_setores[col] = df_setores[col].astype(str).fillna('N√ÉO INFORMADO')\n",
    "        else:\n",
    "            df_setores[col] = pd.to_numeric(df_setores[col], errors='coerce').fillna(0)\n",
    "\n",
    "    print(f\"\\nTOP {len(df_setores)} SETORES POR VALOR EM COBRAN√áA:\\n\")\n",
    "    for idx, row in df_setores.head(15).iterrows():\n",
    "        print(f\"{idx+1:2d}. Se√ß√£o {row['secao_cnae']}: {row['descricao_secao']}\")\n",
    "        print(f\"    Empresas: {int(row['qtd_empresas']):,} | D√©bitos: {int(row['qtd_debitos']):,}\")\n",
    "        print(f\"    Valor Total: R$ {float(row['valor_total']):,.2f}\")\n",
    "        print(f\"    Valor M√©dio: R$ {float(row['valor_medio']):,.2f}\")\n",
    "        print(f\"    Score M√©dio: {float(row['score_medio']):.2f}\")\n",
    "        print(f\"    Prioridade M√°xima/Alta: {int(row['qtd_max']):,} / {int(row['qtd_alta']):,}\")\n",
    "        print()\n",
    "\n",
    "    # Visualiza√ß√£o\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "    fig.suptitle('An√°lise por Setor Econ√¥mico (V1.4)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    top10_setores = df_setores.head(10).copy()\n",
    "\n",
    "    # 1. Valor por Setor\n",
    "    axes[0, 0].barh(range(len(top10_setores)), top10_setores['valor_total']/1e6, color='coral')\n",
    "    axes[0, 0].set_yticks(range(len(top10_setores)))\n",
    "    axes[0, 0].set_yticklabels([f\"{row['secao_cnae']} - {row['descricao_secao'][:25]}\" \n",
    "                                for _, row in top10_setores.iterrows()])\n",
    "    axes[0, 0].set_xlabel('Valor Total (Milh√µes R$)')\n",
    "    axes[0, 0].set_title('Valor em Cobran√ßa por Setor - Top 10')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # 2. Score M√©dio por Setor\n",
    "    axes[0, 1].barh(range(len(top10_setores)), top10_setores['score_medio'], color='steelblue')\n",
    "    axes[0, 1].set_yticks(range(len(top10_setores)))\n",
    "    axes[0, 1].set_yticklabels([f\"{row['secao_cnae']}\" for _, row in top10_setores.iterrows()])\n",
    "    axes[0, 1].set_xlabel('Score M√©dio de Prioriza√ß√£o')\n",
    "    axes[0, 1].set_title('Score M√©dio por Setor - Top 10')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # 3. Distribui√ß√£o Prioridade Alta por Setor\n",
    "    x = np.arange(len(top10_setores))\n",
    "    width = 0.35\n",
    "    axes[1, 0].bar(x - width/2, top10_setores['qtd_max'], width, label='M√°xima', color='#d62728')\n",
    "    axes[1, 0].bar(x + width/2, top10_setores['qtd_alta'], width, label='Alta', color='#ff7f0e')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels([row['secao_cnae'] for _, row in top10_setores.iterrows()])\n",
    "    axes[1, 0].set_ylabel('Quantidade')\n",
    "    axes[1, 0].set_title('D√©bitos de Prioridade M√°xima/Alta por Setor')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 4. Empresas e Valor M√©dio\n",
    "    ax4_1 = axes[1, 1]\n",
    "    ax4_2 = ax4_1.twinx()\n",
    "    \n",
    "    x = np.arange(len(top10_setores))\n",
    "    ax4_1.bar(x - 0.2, top10_setores['qtd_empresas'], 0.4, label='Empresas', color='green', alpha=0.7)\n",
    "    ax4_2.bar(x + 0.2, top10_setores['valor_medio']/1000, 0.4, label='Valor M√©dio (Mil R$)', color='purple', alpha=0.7)\n",
    "    \n",
    "    ax4_1.set_xticks(x)\n",
    "    ax4_1.set_xticklabels([row['secao_cnae'] for _, row in top10_setores.iterrows()])\n",
    "    ax4_1.set_ylabel('Quantidade de Empresas', color='green')\n",
    "    ax4_2.set_ylabel('Valor M√©dio (Mil R$)', color='purple')\n",
    "    ax4_1.set_title('Empresas vs Valor M√©dio por Setor')\n",
    "    ax4_1.tick_params(axis='y', labelcolor='green')\n",
    "    ax4_2.tick_params(axis='y', labelcolor='purple')\n",
    "    ax4_1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3c98e-bc4c-4304-adb4-7f6f9d2bc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 7 FINAL: AN√ÅLISE POR SETOR ECON√îMICO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. AN√ÅLISE POR SETOR ECON√îMICO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query_setores = \"\"\"\n",
    "WITH dados_unicos AS (\n",
    "    SELECT DISTINCT\n",
    "        fm.inscricao_estadual,\n",
    "        fm.secao_cnae,\n",
    "        fm.cnae_divisao,\n",
    "        fm.tipo_debito,\n",
    "        sp.valor_total_devido,\n",
    "        sp.score_final_priorizacao,\n",
    "        sp.classificacao_prioridade,\n",
    "        fm.taxa_efetividade_contatos,\n",
    "        fm.qtd_total_contatos,\n",
    "        fm.score_noteiras,\n",
    "        fm.flag_falencia\n",
    "    FROM gecob.prior_features_ml fm\n",
    "    INNER JOIN gecob.prior_score_priorizacao sp \n",
    "        ON fm.inscricao_estadual = sp.inscricao_estadual\n",
    "        AND fm.tipo_debito = sp.tipo_debito\n",
    "    WHERE fm.secao_cnae IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    secao_cnae,\n",
    "    CASE \n",
    "        WHEN secao_cnae = 'A' THEN 'AGRICULTURA, PECU√ÅRIA'\n",
    "        WHEN secao_cnae = 'B' THEN 'IND√öSTRIAS EXTRATIVAS'\n",
    "        WHEN secao_cnae = 'C' THEN 'IND√öSTRIAS DE TRANSFORMA√á√ÉO'\n",
    "        WHEN secao_cnae = 'D' THEN 'ELETRICIDADE E G√ÅS'\n",
    "        WHEN secao_cnae = 'E' THEN '√ÅGUA, ESGOTO, RES√çDUOS'\n",
    "        WHEN secao_cnae = 'F' THEN 'CONSTRU√á√ÉO'\n",
    "        WHEN secao_cnae = 'G' THEN 'COM√âRCIO'\n",
    "        WHEN secao_cnae = 'H' THEN 'TRANSPORTE, ARMAZENAGEM'\n",
    "        WHEN secao_cnae = 'I' THEN 'ALOJAMENTO E ALIMENTA√á√ÉO'\n",
    "        WHEN secao_cnae = 'J' THEN 'INFORMA√á√ÉO E COMUNICA√á√ÉO'\n",
    "        WHEN secao_cnae = 'K' THEN 'ATIVIDADES FINANCEIRAS'\n",
    "        WHEN secao_cnae = 'L' THEN 'ATIVIDADES IMOBILI√ÅRIAS'\n",
    "        WHEN secao_cnae = 'M' THEN 'ATIVIDADES PROFISSIONAIS'\n",
    "        WHEN secao_cnae = 'N' THEN 'ATIVIDADES ADMINISTRATIVAS'\n",
    "        WHEN secao_cnae = 'O' THEN 'ADMINISTRA√á√ÉO P√öBLICA'\n",
    "        WHEN secao_cnae = 'P' THEN 'EDUCA√á√ÉO'\n",
    "        WHEN secao_cnae = 'Q' THEN 'SA√öDE HUMANA'\n",
    "        WHEN secao_cnae = 'R' THEN 'ARTES, CULTURA, ESPORTE'\n",
    "        WHEN secao_cnae = 'S' THEN 'OUTRAS ATIVIDADES'\n",
    "        ELSE secao_cnae\n",
    "    END AS descricao_secao,\n",
    "    \n",
    "    COUNT(DISTINCT inscricao_estadual) AS qtd_empresas,\n",
    "    COUNT(*) AS qtd_debitos,\n",
    "    SUM(valor_total_devido) AS valor_total,\n",
    "    AVG(valor_total_devido) AS valor_medio,\n",
    "    AVG(score_final_priorizacao) AS score_medio,\n",
    "    \n",
    "    SUM(CASE WHEN classificacao_prioridade = 'PRIORIDADE_MAXIMA' THEN 1 ELSE 0 END) AS qtd_max,\n",
    "    SUM(CASE WHEN classificacao_prioridade = 'PRIORIDADE_ALTA' THEN 1 ELSE 0 END) AS qtd_alta,\n",
    "    \n",
    "    AVG(taxa_efetividade_contatos) AS media_taxa_resposta,\n",
    "    AVG(qtd_total_contatos) AS media_contatos,\n",
    "    AVG(score_noteiras) AS media_score_noteiras,\n",
    "    SUM(CASE WHEN flag_falencia = 1 THEN 1 ELSE 0 END) AS qtd_falencia\n",
    "\n",
    "FROM dados_unicos\n",
    "GROUP BY secao_cnae\n",
    "HAVING COUNT(DISTINCT inscricao_estadual) >= 100\n",
    "ORDER BY valor_total DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "df_setores = spark.sql(query_setores).toPandas()\n",
    "\n",
    "if len(df_setores) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  AVISO: Nenhum setor encontrado!\")\n",
    "else:\n",
    "    # Converter dados\n",
    "    for col in df_setores.columns:\n",
    "        if col in ['secao_cnae', 'descricao_secao']:\n",
    "            df_setores[col] = df_setores[col].astype(str).fillna('N√ÉO INFORMADO')\n",
    "        else:\n",
    "            df_setores[col] = pd.to_numeric(df_setores[col], errors='coerce').fillna(0)\n",
    "\n",
    "    print(f\"\\nTOP {len(df_setores)} SETORES POR VALOR EM COBRAN√áA:\\n\")\n",
    "    for idx, row in df_setores.head(15).iterrows():\n",
    "        print(f\"{idx+1:2d}. Se√ß√£o {row['secao_cnae']}: {row['descricao_secao']}\")\n",
    "        print(f\"    Empresas: {int(row['qtd_empresas']):,} | D√©bitos: {int(row['qtd_debitos']):,}\")\n",
    "        print(f\"    Valor Total: R$ {float(row['valor_total']):,.2f}\")\n",
    "        print(f\"    Valor M√©dio: R$ {float(row['valor_medio']):,.2f}\")\n",
    "        print(f\"    Score M√©dio: {float(row['score_medio']):.2f}\")\n",
    "        print(f\"    Prioridade M√°xima/Alta: {int(row['qtd_max']):,} / {int(row['qtd_alta']):,}\")\n",
    "        print()\n",
    "\n",
    "    # Visualiza√ß√£o\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "    fig.suptitle('An√°lise por Setor Econ√¥mico', fontsize=16, fontweight='bold')\n",
    "\n",
    "    top10_setores = df_setores.head(10).copy()\n",
    "\n",
    "    # 1. Valor por Setor\n",
    "    axes[0, 0].barh(range(len(top10_setores)), top10_setores['valor_total']/1e6, color='coral')\n",
    "    axes[0, 0].set_yticks(range(len(top10_setores)))\n",
    "    axes[0, 0].set_yticklabels([f\"{row['secao_cnae']} - {row['descricao_secao'][:25]}\" \n",
    "                                for _, row in top10_setores.iterrows()])\n",
    "    axes[0, 0].set_xlabel('Valor Total (Milh√µes R$)')\n",
    "    axes[0, 0].set_title('Valor em Cobran√ßa por Setor - Top 10')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # 2. Score M√©dio por Setor\n",
    "    axes[0, 1].barh(range(len(top10_setores)), top10_setores['score_medio'], color='steelblue')\n",
    "    axes[0, 1].set_yticks(range(len(top10_setores)))\n",
    "    axes[0, 1].set_yticklabels([f\"{row['secao_cnae']}\" for _, row in top10_setores.iterrows()])\n",
    "    axes[0, 1].set_xlabel('Score M√©dio de Prioriza√ß√£o')\n",
    "    axes[0, 1].set_title('Score M√©dio por Setor - Top 10')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # 3. Distribui√ß√£o Prioridade Alta por Setor\n",
    "    x = np.arange(len(top10_setores))\n",
    "    width = 0.35\n",
    "    axes[1, 0].bar(x - width/2, top10_setores['qtd_max'], width, label='M√°xima', color='#d62728')\n",
    "    axes[1, 0].bar(x + width/2, top10_setores['qtd_alta'], width, label='Alta', color='#ff7f0e')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels([row['secao_cnae'] for _, row in top10_setores.iterrows()])\n",
    "    axes[1, 0].set_ylabel('Quantidade')\n",
    "    axes[1, 0].set_title('D√©bitos de Prioridade M√°xima/Alta por Setor')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 4. Empresas e Valor M√©dio\n",
    "    ax4_1 = axes[1, 1]\n",
    "    ax4_2 = ax4_1.twinx()\n",
    "    \n",
    "    x = np.arange(len(top10_setores))\n",
    "    ax4_1.bar(x - 0.2, top10_setores['qtd_empresas'], 0.4, label='Empresas', color='green', alpha=0.7)\n",
    "    ax4_2.bar(x + 0.2, top10_setores['valor_medio']/1000, 0.4, label='Valor M√©dio (Mil R$)', color='purple', alpha=0.7)\n",
    "    \n",
    "    ax4_1.set_xticks(x)\n",
    "    ax4_1.set_xticklabels([row['secao_cnae'] for _, row in top10_setores.iterrows()])\n",
    "    ax4_1.set_ylabel('Quantidade de Empresas', color='green')\n",
    "    ax4_2.set_ylabel('Valor M√©dio (Mil R$)', color='purple')\n",
    "    ax4_1.set_title('Empresas vs Valor M√©dio por Setor')\n",
    "    ax4_1.tick_params(axis='y', labelcolor='green')\n",
    "    ax4_2.tick_params(axis='y', labelcolor='purple')\n",
    "    ax4_1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f22d8a-ac67-470f-a751-5a0d1c4d48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AN√ÅLISE POR MUNIC√çPIO (CORRIGIDA - SEM CONFLITOS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. AN√ÅLISE POR MUNIC√çPIO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# IMPORTANTE: Importar builtins\n",
    "import builtins\n",
    "\n",
    "query_municipios = \"\"\"\n",
    "WITH dados_unicos AS (\n",
    "    SELECT DISTINCT\n",
    "        fm.inscricao_estadual,\n",
    "        fm.codigo_municipio,\n",
    "        fm.codigo_uf,\n",
    "        fm.tipo_debito,\n",
    "        sp.valor_total_devido,\n",
    "        sp.score_final_priorizacao,\n",
    "        sp.classificacao_prioridade,\n",
    "        fm.taxa_efetividade_contatos,\n",
    "        fm.flag_falencia,\n",
    "        fm.flag_devedor_contumaz\n",
    "    FROM gecob.prior_features_ml fm\n",
    "    INNER JOIN gecob.prior_score_priorizacao sp \n",
    "        ON fm.inscricao_estadual = sp.inscricao_estadual\n",
    "        AND fm.tipo_debito = sp.tipo_debito\n",
    "    WHERE fm.codigo_municipio IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    codigo_municipio,\n",
    "    codigo_uf AS uf,\n",
    "    COUNT(DISTINCT inscricao_estadual) AS qtd_empresas,\n",
    "    COUNT(*) AS qtd_debitos,\n",
    "    SUM(valor_total_devido) AS valor_total,\n",
    "    AVG(valor_total_devido) AS valor_medio,\n",
    "    AVG(score_final_priorizacao) AS score_medio,\n",
    "    \n",
    "    SUM(CASE WHEN classificacao_prioridade IN ('PRIORIDADE_MAXIMA', 'PRIORIDADE_ALTA') THEN 1 ELSE 0 END) AS qtd_prioridade_alta,\n",
    "    AVG(taxa_efetividade_contatos) AS media_taxa_resposta,\n",
    "    SUM(CASE WHEN flag_falencia = 1 THEN 1 ELSE 0 END) AS qtd_falencia,\n",
    "    SUM(CASE WHEN flag_devedor_contumaz = 1 THEN 1 ELSE 0 END) AS qtd_contumaz\n",
    "\n",
    "FROM dados_unicos\n",
    "GROUP BY codigo_municipio, codigo_uf\n",
    "HAVING COUNT(DISTINCT inscricao_estadual) >= 50\n",
    "ORDER BY valor_total DESC\n",
    "LIMIT 30\n",
    "\"\"\"\n",
    "\n",
    "df_municipios = spark.sql(query_municipios).toPandas()\n",
    "\n",
    "if len(df_municipios) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  AVISO: Nenhum munic√≠pio encontrado!\")\n",
    "else:\n",
    "    # Converter dados\n",
    "    for col in df_municipios.columns:\n",
    "        if col in ['uf']:\n",
    "            df_municipios[col] = df_municipios[col].astype(str).fillna('SC')\n",
    "        else:\n",
    "            df_municipios[col] = pd.to_numeric(df_municipios[col], errors='coerce').fillna(0)\n",
    "\n",
    "    # CORRIGIDO: Usar builtins.min\n",
    "    print(f\"\\nTOP {builtins.min(20, len(df_municipios))} MUNIC√çPIOS POR VALOR EM COBRAN√áA:\\n\")\n",
    "    for idx, row in df_municipios.head(20).iterrows():\n",
    "        print(f\"{idx+1:2d}. Munic√≠pio: {int(row['codigo_municipio'])} - {row['uf']}\")\n",
    "        print(f\"    Empresas: {int(row['qtd_empresas']):,} | D√©bitos: {int(row['qtd_debitos']):,}\")\n",
    "        print(f\"    Valor Total: R$ {float(row['valor_total']):,.2f}\")\n",
    "        print(f\"    Valor M√©dio: R$ {float(row['valor_medio']):,.2f}\")\n",
    "        print(f\"    Score M√©dio: {float(row['score_medio']):.2f}\")\n",
    "        print(f\"    Prioridade Alta: {int(row['qtd_prioridade_alta']):,}\")\n",
    "        print()\n",
    "\n",
    "    # Visualiza√ß√£o\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    fig.suptitle('An√°lise por Munic√≠pio - Top 15 (V1.4)', fontsize=16, fontweight='bold')\n",
    "\n",
    "    top15 = df_municipios.head(15).copy()\n",
    "    top15['municipio_label'] = top15['codigo_municipio'].astype(int).astype(str)\n",
    "\n",
    "    # 1. Valor por Munic√≠pio\n",
    "    axes[0, 0].barh(range(len(top15)), top15['valor_total']/1e6, color='teal')\n",
    "    axes[0, 0].set_yticks(range(len(top15)))\n",
    "    axes[0, 0].set_yticklabels(top15['municipio_label'])\n",
    "    axes[0, 0].set_xlabel('Valor Total (Milh√µes R$)')\n",
    "    axes[0, 0].set_title('Valor em Cobran√ßa por Munic√≠pio')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # 2. Quantidade de Empresas\n",
    "    axes[0, 1].barh(range(len(top15)), top15['qtd_empresas'], color='olive')\n",
    "    axes[0, 1].set_yticks(range(len(top15)))\n",
    "    axes[0, 1].set_yticklabels(top15['municipio_label'])\n",
    "    axes[0, 1].set_xlabel('Quantidade de Empresas')\n",
    "    axes[0, 1].set_title('Empresas em Cobran√ßa por Munic√≠pio')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "    # 3. Score M√©dio\n",
    "    axes[1, 0].bar(range(len(top15)), top15['score_medio'], color='steelblue')\n",
    "    axes[1, 0].set_xticks(range(len(top15)))\n",
    "    axes[1, 0].set_xticklabels(range(1, len(top15)+1))\n",
    "    axes[1, 0].set_ylabel('Score M√©dio')\n",
    "    axes[1, 0].set_title('Score M√©dio de Prioriza√ß√£o por Munic√≠pio')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 4. Prioridades Altas\n",
    "    axes[1, 1].bar(range(len(top15)), top15['qtd_prioridade_alta'], color='darkred')\n",
    "    axes[1, 1].set_xticks(range(len(top15)))\n",
    "    axes[1, 1].set_xticklabels(range(1, len(top15)+1))\n",
    "    axes[1, 1].set_ylabel('Quantidade')\n",
    "    axes[1, 1].set_title('D√©bitos de Prioridade Alta por Munic√≠pio')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea0cb8-e7d9-4684-a913-5a100255a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 9: AN√ÅLISE DE CORRELA√á√ïES (CORRIGIDA)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"6. AN√ÅLISE DE CORRELA√á√ïES E ESTAT√çSTICAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar quais colunas existem\n",
    "print(\"\\nVerificando colunas dispon√≠veis em prior_features_ml...\")\n",
    "colunas_disponiveis = spark.sql(\"DESCRIBE gecob.prior_features_ml\").toPandas()\n",
    "print(f\"Total de colunas: {len(colunas_disponiveis)}\")\n",
    "\n",
    "# Lista de features que queremos (verificar se existem)\n",
    "features_desejadas = [\n",
    "    'valor_total_devido',\n",
    "    'capacidade_pagamento_estimada',\n",
    "    'taxa_pagamento_vs_devido_6m',\n",
    "    'taxa_omissao',\n",
    "    'taxa_efetividade_contatos',\n",
    "    'dias_desde_inclusao_cobranca',\n",
    "    'score_risco_situacao',\n",
    "    'score_noteiras',\n",
    "    'score_atividade_recente',\n",
    "    'score_compliance',\n",
    "    'qtd_total_contatos',\n",
    "    'qtd_periodos_pagamento_6m',\n",
    "    'qtd_pagamentos_6m',\n",
    "    'valor_total_pago_6m',\n",
    "    'coef_variacao_pagamentos',\n",
    "    'coef_variacao_faturamento',\n",
    "    'aliquota_efetiva_media_6m',\n",
    "    'taxa_divergencia_devido_recolher'\n",
    "]\n",
    "\n",
    "# Verificar quais existem\n",
    "features_existentes = [f for f in features_desejadas \n",
    "                       if f in colunas_disponiveis['col_name'].values]\n",
    "\n",
    "print(f\"Features encontradas: {len(features_existentes)} de {len(features_desejadas)}\")\n",
    "\n",
    "if len(features_existentes) < 5:\n",
    "    print(\"\\n‚ö†Ô∏è  Poucas features dispon√≠veis para an√°lise de correla√ß√£o\")\n",
    "    print(\"Pulando an√°lise de correla√ß√µes...\")\n",
    "else:\n",
    "    # Construir query dinamicamente\n",
    "    features_str = ', '.join(features_existentes)\n",
    "    \n",
    "    query_correlacoes = f\"\"\"\n",
    "    SELECT \n",
    "        {features_str}\n",
    "    FROM gecob.prior_features_ml\n",
    "    WHERE valor_total_devido > 0\n",
    "    LIMIT 5000\n",
    "    \"\"\"\n",
    "    \n",
    "    df_correlacoes = spark.sql(query_correlacoes).toPandas()\n",
    "    \n",
    "    # Converter para num√©rico\n",
    "    for col in df_correlacoes.columns:\n",
    "        df_correlacoes[col] = pd.to_numeric(df_correlacoes[col], errors='coerce')\n",
    "    \n",
    "    df_correlacoes = df_correlacoes.dropna()\n",
    "    \n",
    "    if len(df_correlacoes) < 10:\n",
    "        print(f\"\\n‚ö†Ô∏è  Poucos dados dispon√≠veis ({len(df_correlacoes)} registros)\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Dados carregados: {len(df_correlacoes):,} registros com {len(features_existentes)} features\")\n",
    "        \n",
    "        # Matriz de correla√ß√£o\n",
    "        correlation_matrix = df_correlacoes.corr()\n",
    "        \n",
    "        print(\"\\nTOP 10 CORRELA√á√ïES MAIS FORTES:\\n\")\n",
    "        corr_values = correlation_matrix.unstack()\n",
    "        corr_values = corr_values[corr_values < 1.0]\n",
    "        corr_sorted = corr_values.abs().sort_values(ascending=False)\n",
    "        \n",
    "        for idx, (pair, val) in enumerate(corr_sorted.head(10).items(), 1):\n",
    "            original_val = corr_values[pair]\n",
    "            print(f\"{idx:2d}. {pair[0][:30]:30s} ‚Üî {pair[1][:30]:30s}: {original_val:+.3f}\")\n",
    "        \n",
    "        # Visualiza√ß√£o\n",
    "        if len(features_existentes) >= 5:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "            fig.suptitle('An√°lise de Correla√ß√µes', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Heatmap\n",
    "            sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0,\n",
    "                       cbar_kws={'label': 'Correla√ß√£o'}, ax=axes[0], \n",
    "                       square=True, vmin=-1, vmax=1, linewidths=0.5)\n",
    "            axes[0].set_title('Matriz de Correla√ß√£o')\n",
    "            \n",
    "            # Estat√≠sticas descritivas\n",
    "            axes[1].axis('off')\n",
    "            stats_text = \"ESTAT√çSTICAS DESCRITIVAS:\\n\\n\"\n",
    "            stats_text += df_correlacoes.describe().to_string()\n",
    "            axes[1].text(0.1, 0.5, stats_text[:500], fontsize=8, verticalalignment='center',\n",
    "                        family='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì AN√ÅLISES EXPLORAT√ìRIAS CONCLU√çDAS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df8411-7ab2-4138-85ea-f296d64f54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C√âLULA 10: AN√ÅLISE DE DISTRIBUI√á√ïES (CORRIGIDO)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"7. AN√ÅLISE DE DISTRIBUI√á√ïES DAS FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Features para an√°lise de distribui√ß√£o\n",
    "features_distribuicao = [\n",
    "    'valor_total_devido',\n",
    "    'capacidade_pagamento_estimada',\n",
    "    'taxa_efetividade_contatos',\n",
    "    'score_risco_situacao',\n",
    "    'score_noteiras',\n",
    "    'dias_desde_inclusao_cobranca',\n",
    "    'qtd_total_contatos',\n",
    "    'score_compliance'\n",
    "]\n",
    "\n",
    "# Verificar se df_correlacoes j√° existe e tem dados\n",
    "if 'df_correlacoes' not in locals() or len(df_correlacoes) == 0:\n",
    "    print(\"\\nCarregando dados para an√°lise de distribui√ß√µes...\")\n",
    "    query_dist = \"\"\"\n",
    "    SELECT \n",
    "        valor_total_devido,\n",
    "        capacidade_pagamento_estimada,\n",
    "        taxa_efetividade_contatos,\n",
    "        score_risco_situacao,\n",
    "        score_noteiras,\n",
    "        dias_desde_inclusao_cobranca,\n",
    "        qtd_total_contatos,\n",
    "        score_compliance\n",
    "    FROM gecob.prior_features_ml\n",
    "    WHERE valor_total_devido > 0\n",
    "    LIMIT 10000\n",
    "    \"\"\"\n",
    "    df_dist = spark.sql(query_dist).toPandas()\n",
    "    \n",
    "    # Converter para num√©rico\n",
    "    for col in df_dist.columns:\n",
    "        df_dist[col] = pd.to_numeric(df_dist[col], errors='coerce')\n",
    "    \n",
    "    df_dist = df_dist.dropna()\n",
    "else:\n",
    "    df_dist = df_correlacoes[features_distribuicao].copy()\n",
    "\n",
    "print(f\"\\nDataset para an√°lise: {len(df_dist):,} registros\")\n",
    "\n",
    "# Estat√≠sticas descritivas\n",
    "print(\"\\nESTAT√çSTICAS DESCRITIVAS:\\n\")\n",
    "stats_df = df_dist.describe().T\n",
    "print(stats_df)\n",
    "\n",
    "# Identificar features com vari√¢ncia zero ou muito baixa\n",
    "features_validas = []\n",
    "features_constantes = []\n",
    "\n",
    "for col in features_distribuicao:\n",
    "    std_val = df_dist[col].std()\n",
    "    unique_vals = df_dist[col].nunique()\n",
    "    \n",
    "    if std_val > 0.0001 and unique_vals > 10:\n",
    "        features_validas.append(col)\n",
    "    else:\n",
    "        features_constantes.append(col)\n",
    "        print(f\"\\n‚ö†Ô∏è  Feature '{col}' tem vari√¢ncia muito baixa (std={std_val:.6f}, valores √∫nicos={unique_vals})\")\n",
    "\n",
    "print(f\"\\n‚úì Features v√°lidas para visualiza√ß√£o: {len(features_validas)}\")\n",
    "print(f\"‚úó Features exclu√≠das (vari√¢ncia baixa): {len(features_constantes)}\")\n",
    "\n",
    "if len(features_validas) == 0:\n",
    "    print(\"\\n‚ùå Nenhuma feature v√°lida para an√°lise de distribui√ß√£o!\")\n",
    "else:\n",
    "    # Visualiza√ß√£o apenas das features v√°lidas\n",
    "    n_features = len(features_validas)\n",
    "    n_rows = (n_features + 1) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, 2, figsize=(18, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    fig.suptitle('Distribui√ß√£o das Principais Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "    for idx, col in enumerate(features_validas):\n",
    "        row = idx // 2\n",
    "        col_idx = idx % 2\n",
    "        \n",
    "        data_col = df_dist[col].dropna()\n",
    "        \n",
    "        # Histograma\n",
    "        axes[row, col_idx].hist(data_col, bins=50, alpha=0.7, \n",
    "                                color='steelblue', edgecolor='black', density=True)\n",
    "        \n",
    "        # Tentar adicionar KDE apenas se a vari√¢ncia for suficiente\n",
    "        try:\n",
    "            if data_col.std() > 0.01 and data_col.nunique() > 20:\n",
    "                data_col.plot(kind='kde', ax=axes[row, col_idx], color='red', linewidth=2)\n",
    "        except Exception as e:\n",
    "            print(f\"  Aviso: N√£o foi poss√≠vel gerar KDE para '{col}'\")\n",
    "        \n",
    "        axes[row, col_idx].set_title(f'Distribui√ß√£o: {col}', fontweight='bold')\n",
    "        axes[row, col_idx].set_xlabel(col)\n",
    "        axes[row, col_idx].set_ylabel('Densidade/Frequ√™ncia')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Adicionar estat√≠sticas\n",
    "        mean_val = data_col.mean()\n",
    "        median_val = data_col.median()\n",
    "        \n",
    "        axes[row, col_idx].axvline(mean_val, color='green', linestyle='--', \n",
    "                                   linewidth=2, label=f'M√©dia: {mean_val:.2f}', alpha=0.7)\n",
    "        axes[row, col_idx].axvline(median_val, color='orange', linestyle='--', \n",
    "                                   linewidth=2, label=f'Mediana: {median_val:.2f}', alpha=0.7)\n",
    "        axes[row, col_idx].legend(fontsize=9)\n",
    "    \n",
    "    # Remover subplots vazios se houver n√∫mero √≠mpar de features\n",
    "    if n_features % 2 == 1:\n",
    "        fig.delaxes(axes[n_rows-1, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# An√°lise adicional: Boxplots\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AN√ÅLISE DE OUTLIERS (BOXPLOTS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(features_validas) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('An√°lise de Outliers - Boxplots', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Selecionar top 4 features mais importantes\n",
    "    top_features = ['valor_total_devido', 'dias_desde_inclusao_cobranca', \n",
    "                    'qtd_total_contatos', 'score_risco_situacao']\n",
    "    \n",
    "    valid_top_features = [f for f in top_features if f in features_validas]\n",
    "    \n",
    "    for idx, feature in enumerate(valid_top_features[:4]):\n",
    "        row = idx // 2\n",
    "        col = idx % 2\n",
    "        \n",
    "        data_feature = df_dist[feature].dropna()\n",
    "        \n",
    "        # Boxplot\n",
    "        bp = axes[row, col].boxplot([data_feature], vert=True, patch_artist=True,\n",
    "                                     boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                                     medianprops=dict(color='red', linewidth=2),\n",
    "                                     whiskerprops=dict(linewidth=1.5),\n",
    "                                     capprops=dict(linewidth=1.5))\n",
    "        \n",
    "        axes[row, col].set_title(f'{feature}', fontweight='bold')\n",
    "        axes[row, col].set_ylabel('Valor')\n",
    "        axes[row, col].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Estat√≠sticas\n",
    "        q1 = data_feature.quantile(0.25)\n",
    "        q3 = data_feature.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        outliers_count = ((data_feature < (q1 - 1.5*iqr)) | (data_feature > (q3 + 1.5*iqr))).sum()\n",
    "        \n",
    "        stats_text = f'Q1: {q1:.2f}\\nMediana: {data_feature.median():.2f}\\nQ3: {q3:.2f}\\nOutliers: {outliers_count}'\n",
    "        axes[row, col].text(0.02, 0.98, stats_text, transform=axes[row, col].transAxes,\n",
    "                           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "                           fontsize=9)\n",
    "    \n",
    "    # Remover subplots vazios\n",
    "    for idx in range(len(valid_top_features), 4):\n",
    "        row = idx // 2\n",
    "        col = idx % 2\n",
    "        fig.delaxes(axes[row, col])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# An√°lise de valores √∫nicos\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMO DE VALORES √öNICOS POR FEATURE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in features_distribuicao:\n",
    "    unique_count = df_dist[col].nunique()\n",
    "    zero_count = (df_dist[col] == 0).sum()\n",
    "    null_count = df_dist[col].isna().sum()\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Valores √∫nicos: {unique_count}\")\n",
    "    print(f\"  Valores zero: {zero_count} ({zero_count/len(df_dist)*100:.1f}%)\")\n",
    "    print(f\"  Valores nulos: {null_count} ({null_count/len(df_dist)*100:.1f}%)\")\n",
    "    print(f\"  Range: [{df_dist[col].min():.2f}, {df_dist[col].max():.2f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729629d7-bc91-47f2-983d-43c5ae55e133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGN√ìSTICO FINAL DO SISTEMA DE PRIORIZA√á√ÉO (ATUALIZADA V1.4)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIAGN√ìSTICO FINAL DO SISTEMA DE PRIORIZA√á√ÉO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar estado das tabelas principais\n",
    "print(\"\\n1. ESTADO DAS TABELAS:\\n\")\n",
    "\n",
    "tabelas_check = {\n",
    "    'gecob.prior_master_consolidado': 'Tabela principal consolidada (SEM DUPLICATAS V1.4)',\n",
    "    'gecob.prior_features_ml': 'Features para Machine Learning (OTIMIZADA V1.4)',\n",
    "    'gecob.prior_score_priorizacao': 'Scores de prioriza√ß√£o',\n",
    "    'gecob.prior_score_componentes': 'Componentes dos scores'\n",
    "}\n",
    "\n",
    "for tabela, descricao in tabelas_check.items():\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {tabela}\").collect()[0]['cnt']\n",
    "        empresas = spark.sql(f\"SELECT COUNT(DISTINCT inscricao_estadual) as cnt FROM {tabela}\").collect()[0]['cnt']\n",
    "        status = \"‚úÖ OK\" if count > 100 else \"‚ö†Ô∏è  POUCOS DADOS\"\n",
    "        print(f\"{status} {tabela}\")\n",
    "        print(f\"   {descricao}\")\n",
    "        print(f\"   Registros: {count:,} | Empresas: {empresas:,}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {tabela}\")\n",
    "        print(f\"   ERRO: {str(e)[:100]}\")\n",
    "        print()\n",
    "\n",
    "# An√°lise de variabilidade dos dados - ATUALIZADA V1.4\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. AN√ÅLISE DE VARIABILIDADE E ESTAT√çSTICAS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "variability_check = \"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT mc.inscricao_estadual) as empresas_unicas,\n",
    "    COUNT(*) as total_registros,\n",
    "    COUNT(DISTINCT CONCAT(mc.inscricao_estadual, '|', mc.tipo_debito)) as chaves_unicas,\n",
    "    COUNT(DISTINCT mc.tipo_debito) as tipos_debito,\n",
    "    COUNT(DISTINCT mc.porte_por_faturamento) as portes,\n",
    "    COUNT(DISTINCT mc.secao_cnae) as setores,\n",
    "    COUNT(DISTINCT mc.nome_municipio) as municipios,\n",
    "    COUNT(DISTINCT mc.status_debito) as status,\n",
    "    COUNT(DISTINCT sp.classificacao_prioridade) as prioridades,\n",
    "    \n",
    "    CAST(MIN(mc.valor_total_devido) AS DOUBLE) as min_valor,\n",
    "    CAST(MAX(mc.valor_total_devido) AS DOUBLE) as max_valor,\n",
    "    CAST(MIN(DATEDIFF(CURRENT_DATE(), mc.data_inclusao_cobranca)) AS INT) as min_dias,\n",
    "    CAST(MAX(DATEDIFF(CURRENT_DATE(), mc.data_inclusao_cobranca)) AS INT) as max_dias,\n",
    "    \n",
    "    SUM(CASE WHEN mc.flag_falencia = 1 THEN 1 ELSE 0 END) as qtd_falencia,\n",
    "    SUM(CASE WHEN mc.flag_recuperacao_judicial = 1 THEN 1 ELSE 0 END) as qtd_recuperacao,\n",
    "    SUM(CASE WHEN mc.flag_devedor_contumaz = 1 THEN 1 ELSE 0 END) as qtd_contumaz,\n",
    "    \n",
    "    CAST(AVG(sp.score_final_priorizacao) AS DOUBLE) as score_medio,\n",
    "    CAST(MIN(sp.score_final_priorizacao) AS DOUBLE) as score_min,\n",
    "    CAST(MAX(sp.score_final_priorizacao) AS DOUBLE) as score_max,\n",
    "    \n",
    "    SUM(CASE WHEN sp.classificacao_prioridade = 'PRIORIDADE_MAXIMA' THEN 1 ELSE 0 END) as qtd_max,\n",
    "    SUM(CASE WHEN sp.classificacao_prioridade = 'PRIORIDADE_ALTA' THEN 1 ELSE 0 END) as qtd_alta,\n",
    "    SUM(CASE WHEN sp.classificacao_prioridade = 'PRIORIDADE_MEDIA' THEN 1 ELSE 0 END) as qtd_media,\n",
    "    SUM(CASE WHEN sp.classificacao_prioridade = 'PRIORIDADE_BAIXA' THEN 1 ELSE 0 END) as qtd_baixa,\n",
    "    \n",
    "    CAST(SUM(mc.valor_total_devido) AS DOUBLE) as valor_total_cobranca,\n",
    "    \n",
    "    -- NOVAS M√âTRICAS V1.4\n",
    "    CAST(AVG(COALESCE(mc.qtd_contatos_ultimos_30_dias, 0)) AS DOUBLE) as media_contatos_30d,\n",
    "    CAST(AVG(COALESCE(mc.qtd_contatos_ultimos_90_dias, 0)) AS DOUBLE) as media_contatos_90d\n",
    "    \n",
    "FROM gecob.prior_master_consolidado mc\n",
    "INNER JOIN gecob.prior_score_priorizacao sp \n",
    "    ON mc.inscricao_estadual = sp.inscricao_estadual\n",
    "    AND mc.tipo_debito = sp.tipo_debito\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    var_result = spark.sql(variability_check).collect()[0]\n",
    "    \n",
    "    # Converter para dicion√°rio Python comum para facilitar\n",
    "    dados = {k: var_result[k] for k in var_result.asDict().keys()}\n",
    "    \n",
    "    print(f\"\\nüìä DIMENS√ïES DO DATASET:\")\n",
    "    print(f\"  Empresas √∫nicas: {dados['empresas_unicas']:,}\")\n",
    "    print(f\"  Total de d√©bitos: {dados['total_registros']:,}\")\n",
    "    print(f\"  Chaves √∫nicas (IE+Tipo): {dados['chaves_unicas']:,}\")\n",
    "    \n",
    "    # VERIFICA√á√ÉO DE DUPLICATAS V1.4\n",
    "    if dados['total_registros'] == dados['chaves_unicas']:\n",
    "        print(f\"  ‚úÖ SEM DUPLICATAS (vers√£o V1.4 funcionando corretamente)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  ATEN√á√ÉO: {dados['total_registros'] - dados['chaves_unicas']:,} poss√≠veis duplicatas!\")\n",
    "    \n",
    "    print(f\"  D√©bitos por empresa: {dados['total_registros']/dados['empresas_unicas']:.1f}\")\n",
    "    print(f\"  Tipos de d√©bito: {dados['tipos_debito']}\")\n",
    "    print(f\"  Portes: {dados['portes']}\")\n",
    "    print(f\"  Setores CNAE: {dados['setores']}\")\n",
    "    print(f\"  Munic√≠pios: {dados['municipios']}\")\n",
    "    print(f\"  Status distintos: {dados['status']}\")\n",
    "    print(f\"  N√≠veis de prioridade: {dados['prioridades']}\")\n",
    "    \n",
    "    print(f\"\\nüí∞ VALORES EM COBRAN√áA:\")\n",
    "    valor_bi = dados['valor_total_cobranca'] / 1e9\n",
    "    valor_medio = dados['valor_total_cobranca'] / dados['total_registros']\n",
    "    print(f\"  Valor Total: R$ {dados['valor_total_cobranca']:,.2f}\")\n",
    "    print(f\"  Em Bilh√µes: R$ {valor_bi:.2f} bi\")\n",
    "    print(f\"  Valor M√©dio/D√©bito: R$ {valor_medio:,.2f}\")\n",
    "    print(f\"  Range: R$ {dados['min_valor']:,.2f} a R$ {dados['max_valor']:,.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìÖ TEMPO EM COBRAN√áA:\")\n",
    "    print(f\"  M√≠nimo: {dados['min_dias']} dias\")\n",
    "    print(f\"  M√°ximo: {dados['max_dias']} dias\")\n",
    "    print(f\"  M√©dia aprox: {(dados['min_dias'] + dados['max_dias'])/2:.0f} dias\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  FLAGS DE RISCO:\")\n",
    "    print(f\"  Fal√™ncias: {dados['qtd_falencia']:,}\")\n",
    "    print(f\"  Recupera√ß√µes Judiciais: {dados['qtd_recuperacao']:,}\")\n",
    "    print(f\"  Devedores Contumazes: {dados['qtd_contumaz']:,}\")\n",
    "    total_risco = dados['qtd_falencia'] + dados['qtd_recuperacao'] + dados['qtd_contumaz']\n",
    "    pct_risco = total_risco / dados['empresas_unicas'] * 100\n",
    "    print(f\"  Total com risco: {total_risco:,} ({pct_risco:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüéØ SCORES DE PRIORIZA√á√ÉO:\")\n",
    "    print(f\"  M√©dia: {dados['score_medio']:.2f}\")\n",
    "    print(f\"  Range: {dados['score_min']:.2f} a {dados['score_max']:.2f}\")\n",
    "    print(f\"  Amplitude: {dados['score_max'] - dados['score_min']:.2f} pontos\")\n",
    "    \n",
    "    print(f\"\\nüìà DISTRIBUI√á√ÉO POR PRIORIDADE:\")\n",
    "    pct_max = dados['qtd_max'] / dados['total_registros'] * 100\n",
    "    pct_alta = dados['qtd_alta'] / dados['total_registros'] * 100\n",
    "    pct_media = dados['qtd_media'] / dados['total_registros'] * 100\n",
    "    pct_baixa = dados['qtd_baixa'] / dados['total_registros'] * 100\n",
    "    \n",
    "    print(f\"  M√ÅXIMA: {dados['qtd_max']:,} ({pct_max:.1f}%)\")\n",
    "    print(f\"  ALTA:   {dados['qtd_alta']:,} ({pct_alta:.1f}%)\")\n",
    "    print(f\"  M√âDIA:  {dados['qtd_media']:,} ({pct_media:.1f}%)\")\n",
    "    print(f\"  BAIXA:  {dados['qtd_baixa']:,} ({pct_baixa:.1f}%)\")\n",
    "    \n",
    "    # NOVAS M√âTRICAS V1.4\n",
    "    print(f\"\\nüìû ATIVIDADE DE CONTATO (V1.4):\")\n",
    "    print(f\"  M√©dia contatos √∫ltimos 30 dias: {dados['media_contatos_30d']:.2f}\")\n",
    "    print(f\"  M√©dia contatos √∫ltimos 90 dias: {dados['media_contatos_90d']:.2f}\")\n",
    "    \n",
    "    # Visualiza√ß√£o\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    fig.suptitle('Dashboard - Sistema de Prioriza√ß√£o GECOB/SC (V1.4)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Pizza - Distribui√ß√£o por Prioridade\n",
    "    prioridades = ['M√ÅXIMA', 'ALTA', 'M√âDIA', 'BAIXA']\n",
    "    valores_prior = [dados['qtd_max'], dados['qtd_alta'], dados['qtd_media'], dados['qtd_baixa']]\n",
    "    colors = ['#8b0000', '#d62728', '#ff7f0e', '#2ca02c']\n",
    "    axes[0, 0].pie(valores_prior, labels=prioridades, autopct='%1.1f%%', \n",
    "                   colors=colors, startangle=90)\n",
    "    axes[0, 0].set_title('Distribui√ß√£o de D√©bitos por Prioridade')\n",
    "    \n",
    "    # 2. Texto - Estat√≠sticas\n",
    "    axes[0, 1].axis('off')\n",
    "    stats_text = f\"\"\"\n",
    "RESUMO EXECUTIVO (V1.4)\n",
    "\n",
    "Volume:\n",
    "  ‚Ä¢ {dados['empresas_unicas']:,} empresas\n",
    "  ‚Ä¢ {dados['total_registros']:,} d√©bitos\n",
    "  ‚Ä¢ R$ {valor_bi:.1f} bilh√µes\n",
    "  ‚Ä¢ {dados['chaves_unicas']:,} chaves √∫nicas\n",
    "  \n",
    "Prioriza√ß√£o:\n",
    "  ‚Ä¢ Score: {dados['score_medio']:.1f} (m√©dia)\n",
    "  ‚Ä¢ Range: {dados['score_min']:.0f}-{dados['score_max']:.0f}\n",
    "  ‚Ä¢ Alta prioridade: {dados['qtd_max']+dados['qtd_alta']:,}\n",
    "  ‚Ä¢ % Imediato: {(pct_max+pct_alta):.1f}%\n",
    "  \n",
    "Risco:\n",
    "  ‚Ä¢ Total: {total_risco:,} empresas\n",
    "  ‚Ä¢ % do total: {pct_risco:.1f}%\n",
    "  \n",
    "Cobertura:\n",
    "  ‚Ä¢ {dados['municipios']} munic√≠pios\n",
    "  ‚Ä¢ {dados['setores']} setores\n",
    "  ‚Ä¢ {dados['tipos_debito']} tipos d√©bito\n",
    "\n",
    "Contatos (V1.4):\n",
    "  ‚Ä¢ √öltimos 30d: {dados['media_contatos_30d']:.2f}\n",
    "  ‚Ä¢ √öltimos 90d: {dados['media_contatos_90d']:.2f}\n",
    "    \"\"\"\n",
    "    axes[0, 1].text(0.05, 0.5, stats_text, fontsize=10, verticalalignment='center',\n",
    "                   family='monospace', \n",
    "                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "    \n",
    "    # 3. Barras - Flags de Risco\n",
    "    if total_risco > 0:\n",
    "        risk_cats = ['Fal√™ncia', 'Recupera√ß√£o\\nJudicial', 'Devedor\\nContumaz']\n",
    "        risk_vals = [dados['qtd_falencia'], dados['qtd_recuperacao'], dados['qtd_contumaz']]\n",
    "        axes[1, 0].bar(risk_cats, risk_vals, color=['#8b0000', '#d62728', '#ff7f0e'])\n",
    "        axes[1, 0].set_ylabel('Quantidade')\n",
    "        axes[1, 0].set_title('Empresas com Flags de Risco')\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "        for i, v in enumerate(risk_vals):\n",
    "            if v > 0:\n",
    "                axes[1, 0].text(i, v, f'{v:,}', ha='center', va='bottom')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'Sem flags de risco', \n",
    "                       ha='center', va='center', fontsize=14, \n",
    "                       transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Empresas com Flags de Risco')\n",
    "    \n",
    "    # 4. Barras horizontais - Valor por Prioridade (estimado)\n",
    "    valor_max = pct_max/100 * dados['valor_total_cobranca'] / 1e9\n",
    "    valor_alta = pct_alta/100 * dados['valor_total_cobranca'] / 1e9\n",
    "    valor_media = pct_media/100 * dados['valor_total_cobranca'] / 1e9\n",
    "    valor_baixa = pct_baixa/100 * dados['valor_total_cobranca'] / 1e9\n",
    "    \n",
    "    valores_bi = [valor_max, valor_alta, valor_media, valor_baixa]\n",
    "    axes[1, 1].barh(prioridades, valores_bi, color=colors)\n",
    "    axes[1, 1].set_xlabel('Valor Estimado (Bilh√µes R$)')\n",
    "    axes[1, 1].set_title('Valor em Cobran√ßa por Prioridade (estimado)')\n",
    "    axes[1, 1].invert_yaxis()\n",
    "    for i, v in enumerate(valores_bi):\n",
    "        axes[1, 1].text(v, i, f' R$ {v:.1f}bi', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Diagn√≥stico\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"3. DIAGN√ìSTICO:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n‚úÖ PONTOS FORTES:\")\n",
    "    print(f\"  ‚Ä¢ Volume expressivo: {dados['empresas_unicas']:,} empresas\")\n",
    "    print(f\"  ‚Ä¢ Alto impacto: R$ {valor_bi:.1f} bilh√µes\")\n",
    "    print(f\"  ‚Ä¢ Sistema operacional: {dados['total_registros']:,} d√©bitos processados\")\n",
    "    print(f\"  ‚Ä¢ ‚úÖ V1.4: Sem duplicatas na master_consolidado\")\n",
    "    print(f\"  ‚Ä¢ ‚úÖ V1.4: M√©tricas temporais de contato implementadas\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  PONTOS DE ATEN√á√ÉO:\")\n",
    "    if dados['setores'] == 0:\n",
    "        print(\"  ‚Ä¢ Dados de CNAE n√£o preenchidos - CR√çTICO\")\n",
    "    if dados['municipios'] == 0:\n",
    "        print(\"  ‚Ä¢ Dados de munic√≠pio n√£o preenchidos - IMPORTANTE\")\n",
    "    if pct_risco < 1:\n",
    "        print(f\"  ‚Ä¢ Poucas flags de risco: {pct_risco:.2f}% - integrar fontes externas\")\n",
    "    \n",
    "    # Recomenda√ß√µes\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"4. RECOMENDA√á√ïES:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüîß IMEDIATAS:\")\n",
    "    if dados['setores'] == 0:\n",
    "        print(\"  1. URGENTE: Enriquecer dados de CNAE via consulta RFB\")\n",
    "    if dados['municipios'] == 0:\n",
    "        print(\"  2. URGENTE: Preencher dados de munic√≠pio no cadastro\")\n",
    "    print(\"  3. Validar sistema com amostra de casos reais\")\n",
    "    print(\"  4. ‚úÖ V1.4: Sistema j√° otimizado (passagem √∫nica features_ml)\")\n",
    "    \n",
    "    print(\"\\nüìÖ CURTO PRAZO (30 dias):\")\n",
    "    print(\"  1. Treinar equipe no uso do sistema\")\n",
    "    print(\"  2. Estabelecer SLAs por n√≠vel de prioridade\")\n",
    "    print(\"  3. Criar relat√≥rios gerenciais\")\n",
    "    print(\"  4. Explorar m√©tricas temporais (30d/90d) para segmenta√ß√£o\")\n",
    "    \n",
    "    print(\"\\nüìà M√âDIO PRAZO (90 dias):\")\n",
    "    print(\"  1. Integrar Serasa/Boa Vista\")\n",
    "    print(\"  2. Implementar feedback loop\")\n",
    "    print(\"  3. Dashboard executivo\")\n",
    "    print(\"  4. An√°lise preditiva com hist√≥rico temporal\")\n",
    "    \n",
    "    # Resumo final\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"5. RESUMO EXECUTIVO:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    impacto_estimado = dados['valor_total_cobranca'] * 0.05\n",
    "    \n",
    "    print(f\"\"\"\n",
    "üéØ SISTEMA DE PRIORIZA√á√ÉO - GECOB/SC (V1.4)\n",
    "\n",
    "STATUS: ‚úÖ OPERACIONAL E OTIMIZADO\n",
    "  ‚Ä¢ {dados['empresas_unicas']:,} empresas processadas\n",
    "  ‚Ä¢ {dados['total_registros']:,} d√©bitos gerenciados\n",
    "  ‚Ä¢ R$ {valor_bi:.2f} bilh√µes em cobran√ßa\n",
    "  ‚Ä¢ {dados['qtd_max']+dados['qtd_alta']:,} casos priorit√°rios\n",
    "  ‚Ä¢ ‚úÖ Sem duplicatas (chave: IE + tipo_debito)\n",
    "\n",
    "MELHORIAS V1.4:\n",
    "  ‚Ä¢ ‚úÖ Conta corrente pr√©-agregada (performance++)\n",
    "  ‚Ä¢ ‚úÖ Features ML em passagem √∫nica (10-12 min)\n",
    "  ‚Ä¢ ‚úÖ M√©tricas temporais de contato (30d/90d)\n",
    "  ‚Ä¢ ‚úÖ Master consolidado sem duplicatas\n",
    "  ‚Ä¢ ‚úÖ JOINs otimizados por IE + tipo_debito\n",
    "\n",
    "POTENCIAL DE IMPACTO:\n",
    "  ‚Ä¢ Recupera√ß√£o incremental: 5-10%\n",
    "  ‚Ä¢ Economia estimada: R$ {impacto_estimado/1e9:.2f} bi/ano\n",
    "  ‚Ä¢ Otimiza√ß√£o de {dados['empresas_unicas']:,} casos\n",
    "  ‚Ä¢ Redu√ß√£o de tempo: 20-30%\n",
    "\n",
    "PR√ìXIMOS PASSOS:\n",
    "  1. ‚úÖ Sistema V1.4 implementado\n",
    "  2. ‚Üí Corre√ß√£o de dados cadastrais (URGENTE)\n",
    "  3. ‚Üí Valida√ß√£o operacional (30 dias)\n",
    "  4. ‚Üí Expans√£o completa (90 dias)\n",
    "    \"\"\")\n",
    "    \n",
    "    success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERRO: {str(e)}\")\n",
    "    success = False\n",
    "    \n",
    "    # Fallback com estat√≠sticas b√°sicas\n",
    "    print(\"\\nCarregando estat√≠sticas b√°sicas...\")\n",
    "    basic = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            CAST(COUNT(*) AS INT) as total,\n",
    "            CAST(COUNT(DISTINCT inscricao_estadual) AS INT) as empresas,\n",
    "            CAST(SUM(valor_total_devido) AS DOUBLE) as valor\n",
    "        FROM gecob.prior_master_consolidado\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"\"\"\n",
    "üìä ESTAT√çSTICAS B√ÅSICAS:\n",
    "  ‚Ä¢ Registros: {basic['total']:,}\n",
    "  ‚Ä¢ Empresas: {basic['empresas']:,}\n",
    "  ‚Ä¢ Valor: R$ {basic['valor']/1e9:.2f} bilh√µes\n",
    "    \"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ DIAGN√ìSTICO FINALIZADO (V1.4)\" if success else \"‚ö†Ô∏è  DIAGN√ìSTICO PARCIAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"üè¢ GECOB - Receita Estadual/SC\")\n",
    "print(\"üìå Vers√£o: SQL V1.4 + Python atualizado\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee306112-23c6-4f3a-96e1-80285c12d286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AN√ÅLISES AVAN√áADAS SEM MACHINE LEARNING SUPERVISIONADO\n",
    "# Sistema de An√°lise Inteligente - GECOB/SC\n",
    "# ============================================================================\n",
    "import builtins  # Para usar fun√ß√µes Python nativas\n",
    "# ============================================================================\n",
    "# CLUSTERING COMPLETO (CORRIGIDA - SEM CONFLITOS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1. CLUSTERING DE EMPRESAS POR PADR√ÉO DE D√âBITO (K-MEANS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# IMPORTANTE: Importar builtins\n",
    "import builtins\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.sql.functions import count as spark_count, avg as spark_avg, sum as spark_sum\n",
    "\n",
    "print(\"\\nCarregando TODOS os dados para clustering...\")\n",
    "\n",
    "# Query para features de clustering - ATUALIZADA V1.4\n",
    "query_clustering = \"\"\"\n",
    "SELECT \n",
    "    mc.inscricao_estadual,\n",
    "    mc.tipo_debito,\n",
    "    CAST(mc.valor_total_devido AS DOUBLE) as valor_total,\n",
    "    CAST(DATEDIFF(CURRENT_DATE(), mc.data_inclusao_cobranca) AS INT) as dias_cobranca,\n",
    "    CAST(mc.qtd_total_contatos AS INT) as qtd_contatos,\n",
    "    CAST(sp.score_final_priorizacao AS DOUBLE) as score_priorizacao,\n",
    "    CAST(mc.saldo_imposto AS DOUBLE) as saldo_imposto,\n",
    "    CAST(mc.saldo_multa AS DOUBLE) as saldo_multa,\n",
    "    CAST(mc.saldo_juros AS DOUBLE) as saldo_juros\n",
    "FROM gecob.prior_master_consolidado mc\n",
    "INNER JOIN gecob.prior_score_priorizacao sp \n",
    "    ON mc.inscricao_estadual = sp.inscricao_estadual\n",
    "    AND mc.tipo_debito = sp.tipo_debito\n",
    "WHERE mc.valor_total_devido > 0\n",
    "    AND mc.valor_total_devido < 10000000\n",
    "\"\"\"\n",
    "\n",
    "df_cluster = spark.sql(query_clustering)\n",
    "total_records = df_cluster.count()\n",
    "print(f\"Dados carregados: {total_records:,} registros\")\n",
    "\n",
    "# Preparar features para clustering\n",
    "features_cols = ['valor_total', 'dias_cobranca', 'qtd_contatos', 'score_priorizacao',\n",
    "                 'saldo_imposto', 'saldo_multa', 'saldo_juros']\n",
    "\n",
    "# Remover nulls\n",
    "df_cluster_clean = df_cluster.na.fill(0)\n",
    "\n",
    "# Assembler\n",
    "assembler = VectorAssembler(inputCols=features_cols, outputCol='features_raw')\n",
    "df_assembled = assembler.transform(df_cluster_clean)\n",
    "\n",
    "# Normalizar\n",
    "scaler = StandardScaler(inputCol='features_raw', outputCol='features', \n",
    "                       withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "# Testar diferentes valores de K\n",
    "print(\"\\nTestando diferentes n√∫meros de clusters...\")\n",
    "silhouette_scores = []\n",
    "k_values = range(3, 8)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(k=k, seed=42, featuresCol='features', predictionCol='cluster', maxIter=20)\n",
    "    model = kmeans.fit(df_scaled)\n",
    "    predictions = model.transform(df_scaled)\n",
    "    \n",
    "    evaluator = ClusteringEvaluator(featuresCol='features', \n",
    "                                    predictionCol='cluster',\n",
    "                                    metricName='silhouette')\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    silhouette_scores.append(silhouette)\n",
    "    print(f\"  K={k}: Silhouette = {silhouette:.4f}\")\n",
    "\n",
    "# Melhor K\n",
    "best_k = k_values[np.argmax(silhouette_scores)]\n",
    "print(f\"\\n‚úÖ Melhor n√∫mero de clusters: K={best_k}\")\n",
    "\n",
    "# Treinar modelo final\n",
    "print(f\"\\nTreinando modelo final com K={best_k}...\")\n",
    "kmeans_final = KMeans(k=best_k, seed=42, featuresCol='features', predictionCol='cluster', maxIter=20)\n",
    "kmeans_model = kmeans_final.fit(df_scaled)\n",
    "df_clustered = kmeans_model.transform(df_scaled)\n",
    "\n",
    "# Analisar clusters\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PERFIS DOS CLUSTERS IDENTIFICADOS:\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "cluster_analysis = df_clustered.groupBy('cluster').agg(\n",
    "    spark_count('*').alias('quantidade'),\n",
    "    spark_avg('valor_total').alias('valor_medio'),\n",
    "    spark_avg('dias_cobranca').alias('dias_medio'),\n",
    "    spark_avg('qtd_contatos').alias('contatos_medio'),\n",
    "    spark_avg('score_priorizacao').alias('score_medio'),\n",
    "    spark_sum('valor_total').alias('valor_total_cluster')\n",
    ").orderBy('cluster').toPandas()\n",
    "\n",
    "# Converter para num√©rico\n",
    "for col in cluster_analysis.columns:\n",
    "    if col != 'cluster':\n",
    "        cluster_analysis[col] = pd.to_numeric(cluster_analysis[col], errors='coerce')\n",
    "\n",
    "# Nomear clusters baseado em caracter√≠sticas\n",
    "def nomear_cluster(row):\n",
    "    if row['valor_medio'] > 50000:\n",
    "        nivel_valor = \"ALTO_VALOR\"\n",
    "    elif row['valor_medio'] > 10000:\n",
    "        nivel_valor = \"MEDIO_VALOR\"\n",
    "    else:\n",
    "        nivel_valor = \"BAIXO_VALOR\"\n",
    "    \n",
    "    if row['dias_medio'] > 365:\n",
    "        nivel_tempo = \"ANTIGO\"\n",
    "    else:\n",
    "        nivel_tempo = \"RECENTE\"\n",
    "    \n",
    "    if row['score_medio'] > 60:\n",
    "        nivel_risco = \"ALTA_PRIOR\"\n",
    "    else:\n",
    "        nivel_risco = \"BAIXA_PRIOR\"\n",
    "    \n",
    "    return f\"{nivel_valor}_{nivel_tempo}_{nivel_risco}\"\n",
    "\n",
    "cluster_analysis['nome_cluster'] = cluster_analysis.apply(nomear_cluster, axis=1)\n",
    "\n",
    "print(f\"Total analisado: {cluster_analysis['quantidade'].sum():,.0f} empresas\\n\")\n",
    "\n",
    "for idx, row in cluster_analysis.iterrows():\n",
    "    print(f\"CLUSTER {int(row['cluster'])}: {row['nome_cluster']}\")\n",
    "    print(f\"  Quantidade: {int(row['quantidade']):,} empresas ({row['quantidade']/cluster_analysis['quantidade'].sum()*100:.1f}%)\")\n",
    "    print(f\"  Valor M√©dio: R$ {row['valor_medio']:,.2f}\")\n",
    "    print(f\"  Valor Total: R$ {row['valor_total_cluster']/1e6:.2f} milh√µes\")\n",
    "    print(f\"  Dias M√©dios em Cobran√ßa: {row['dias_medio']:.0f} dias\")\n",
    "    print(f\"  Contatos M√©dios: {row['contatos_medio']:.1f}\")\n",
    "    print(f\"  Score M√©dio: {row['score_medio']:.1f}\")\n",
    "    print()\n",
    "\n",
    "# Salvar clusters identificados\n",
    "print(\"Salvando resultados do clustering...\")\n",
    "df_clustered.select('inscricao_estadual', 'tipo_debito', 'cluster', 'valor_total', \n",
    "                    'dias_cobranca', 'score_priorizacao').write.mode('overwrite').saveAsTable(\n",
    "    'gecob.prior_clusters_empresas')\n",
    "print(\"‚úÖ Resultados salvos em: gecob.prior_clusters_empresas\\n\")\n",
    "\n",
    "# Visualiza√ß√£o (mantida igual - n√£o precisa corre√ß√£o)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('An√°lise de Clusters - Perfis de Empresas Devedoras (V1.4)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Quantidade por cluster\n",
    "axes[0, 0].bar(cluster_analysis['cluster'].astype(str), cluster_analysis['quantidade'], \n",
    "               color=plt.cm.Set3(np.linspace(0, 1, len(cluster_analysis))))\n",
    "axes[0, 0].set_xlabel('Cluster')\n",
    "axes[0, 0].set_ylabel('Quantidade de Empresas')\n",
    "axes[0, 0].set_title('Distribui√ß√£o de Empresas por Cluster')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(cluster_analysis['quantidade']):\n",
    "    axes[0, 0].text(i, v, f'{int(v):,}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# 2. Valor m√©dio por cluster\n",
    "axes[0, 1].barh(cluster_analysis['nome_cluster'], cluster_analysis['valor_medio']/1000, \n",
    "                color=plt.cm.Set3(np.linspace(0, 1, len(cluster_analysis))))\n",
    "axes[0, 1].set_xlabel('Valor M√©dio (Mil R$)')\n",
    "axes[0, 1].set_title('Valor M√©dio de D√©bito por Cluster')\n",
    "axes[0, 1].invert_yaxis()\n",
    "for i, v in enumerate(cluster_analysis['valor_medio']/1000):\n",
    "    axes[0, 1].text(v, i, f' R$ {v:.1f}k', va='center', fontsize=8)\n",
    "\n",
    "# 3. Pizza - Distribui√ß√£o percentual\n",
    "axes[0, 2].pie(cluster_analysis['quantidade'], \n",
    "               labels=[f\"C{int(c)}\" for c in cluster_analysis['cluster']], \n",
    "               autopct='%1.1f%%',\n",
    "               colors=plt.cm.Set3(np.linspace(0, 1, len(cluster_analysis))))\n",
    "axes[0, 2].set_title('Distribui√ß√£o Percentual')\n",
    "\n",
    "# 4. Scatter: Valor vs Dias (amostra)\n",
    "print(\"Gerando visualiza√ß√£o de scatter (amostra 10%)...\")\n",
    "sample_data = df_clustered.select('valor_total', 'dias_cobranca', 'cluster').sample(\n",
    "    fraction=0.1, seed=42).toPandas()\n",
    "sample_data['valor_total'] = pd.to_numeric(sample_data['valor_total'])\n",
    "sample_data['dias_cobranca'] = pd.to_numeric(sample_data['dias_cobranca'])\n",
    "\n",
    "for cluster_id in cluster_analysis['cluster']:\n",
    "    cluster_data = sample_data[sample_data['cluster'] == cluster_id]\n",
    "    axes[1, 0].scatter(cluster_data['dias_cobranca'], cluster_data['valor_total']/1000,\n",
    "                      alpha=0.5, s=15, label=f'C{int(cluster_id)}')\n",
    "\n",
    "axes[1, 0].set_xlabel('Dias em Cobran√ßa')\n",
    "axes[1, 0].set_ylabel('Valor do D√©bito (Mil R$)')\n",
    "axes[1, 0].set_title('Distribui√ß√£o: Valor vs Tempo (amostra 10%)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Score m√©dio vs Valor total do cluster\n",
    "axes[1, 1].scatter(cluster_analysis['score_medio'], \n",
    "                   cluster_analysis['valor_total_cluster']/1e6,\n",
    "                   s=cluster_analysis['quantidade']/100,\n",
    "                   c=cluster_analysis['cluster'], cmap='Set3',\n",
    "                   alpha=0.6, edgecolors='black', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Score M√©dio de Prioriza√ß√£o')\n",
    "axes[1, 1].set_ylabel('Valor Total do Cluster (Milh√µes R$)')\n",
    "axes[1, 1].set_title('Score vs Valor Total (tamanho = quantidade)')\n",
    "for idx, row in cluster_analysis.iterrows():\n",
    "    axes[1, 1].annotate(f\"C{int(row['cluster'])}\", \n",
    "                       (row['score_medio'], row['valor_total_cluster']/1e6),\n",
    "                       fontsize=10, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Heatmap de caracter√≠sticas m√©dias\n",
    "features_heatmap = cluster_analysis[['cluster', 'valor_medio', 'dias_medio', \n",
    "                                     'contatos_medio', 'score_medio']].set_index('cluster')\n",
    "# Normalizar para heatmap\n",
    "features_norm = (features_heatmap - features_heatmap.min()) / (features_heatmap.max() - features_heatmap.min())\n",
    "im = axes[1, 2].imshow(features_norm.T, cmap='YlOrRd', aspect='auto')\n",
    "axes[1, 2].set_xticks(range(len(cluster_analysis)))\n",
    "axes[1, 2].set_xticklabels([f\"C{int(c)}\" for c in cluster_analysis['cluster']])\n",
    "axes[1, 2].set_yticks(range(len(features_norm.columns)))\n",
    "axes[1, 2].set_yticklabels(['Valor', 'Dias', 'Contatos', 'Score'], fontsize=9)\n",
    "axes[1, 2].set_title('Perfil dos Clusters (normalizado)')\n",
    "plt.colorbar(im, ax=axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"‚úÖ Clustering conclu√≠do com TODOS os dados (V1.4)\")\n",
    "print(f\"  {total_records:,} empresas agrupadas em {best_k} clusters\")\n",
    "# CORRIGIDO: Usar builtins.max\n",
    "print(f\"  Silhouette Score: {builtins.max(silhouette_scores):.4f}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d7645-0aa8-4ced-ab35-59811aa9241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AN√ÅLISE DE OUTLIERS - CASOS AT√çPICOS (CORRIGIDA - SEM CONFLITOS)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. AN√ÅLISE DE OUTLIERS - CASOS AT√çPICOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# IMPORTANTE: Importar fun√ß√µes nativas Python com alias\n",
    "import builtins\n",
    "\n",
    "print(\"\\nIdentificando outliers por m√∫ltiplos crit√©rios...\\n\")\n",
    "\n",
    "# Query para an√°lise de outliers - ATUALIZADA V1.4\n",
    "query_outliers = \"\"\"\n",
    "SELECT \n",
    "    mc.inscricao_estadual,\n",
    "    mc.tipo_debito,\n",
    "    mc.razao_social,\n",
    "    CAST(mc.valor_total_devido AS DOUBLE) as valor_devido,\n",
    "    CAST(DATEDIFF(CURRENT_DATE(), mc.data_inclusao_cobranca) AS INT) as dias_cobranca,\n",
    "    CAST(mc.qtd_total_contatos AS INT) as qtd_contatos,\n",
    "    CAST(sp.score_final_priorizacao AS DOUBLE) as score,\n",
    "    mc.situacao_cadastral_desc,\n",
    "    CAST(mc.flag_falencia AS INT) as flag_falencia,\n",
    "    CAST(mc.flag_recuperacao_judicial AS INT) as flag_recuperacao_judicial,\n",
    "    CAST(mc.flag_devedor_contumaz AS INT) as flag_devedor_contumaz\n",
    "FROM gecob.prior_master_consolidado mc\n",
    "INNER JOIN gecob.prior_score_priorizacao sp \n",
    "    ON mc.inscricao_estadual = sp.inscricao_estadual\n",
    "    AND mc.tipo_debito = sp.tipo_debito\n",
    "WHERE mc.valor_total_devido > 0\n",
    "\"\"\"\n",
    "\n",
    "print(\"Carregando dados...\")\n",
    "df_outliers = spark.sql(query_outliers).toPandas()\n",
    "\n",
    "if len(df_outliers) == 0:\n",
    "    print(\"‚ùå ERRO: Nenhum dado retornado pela query!\")\n",
    "    print(\"Verifique se as tabelas t√™m dados e se o join est√° correto\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dados carregados: {len(df_outliers):,} registros\")\n",
    "    \n",
    "    # Converter para num√©rico\n",
    "    numeric_cols = ['valor_devido', 'dias_cobranca', 'qtd_contatos', 'score']\n",
    "    for col in numeric_cols:\n",
    "        df_outliers[col] = pd.to_numeric(df_outliers[col], errors='coerce')\n",
    "    \n",
    "    # Remover nulls\n",
    "    df_outliers_clean = df_outliers.dropna(subset=numeric_cols)\n",
    "    \n",
    "    if len(df_outliers_clean) == 0:\n",
    "        print(\"‚ùå ERRO: Todos os registros t√™m valores nulos nas features num√©ricas\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Ap√≥s limpeza: {len(df_outliers_clean):,} registros v√°lidos\\n\")\n",
    "        \n",
    "        # Estat√≠sticas b√°sicas\n",
    "        print(\"=\"*80)\n",
    "        print(\"ESTAT√çSTICAS DESCRITIVAS:\")\n",
    "        print(\"=\"*80)\n",
    "        print(df_outliers_clean[numeric_cols].describe())\n",
    "        \n",
    "        # M√©todo 1: IQR (Interquartile Range)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DETEC√á√ÉO POR IQR (Interquartile Range)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        outliers_dict = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            Q1 = df_outliers_clean[col].quantile(0.25)\n",
    "            Q3 = df_outliers_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            if IQR == 0:\n",
    "                print(f\"\\n‚ö†Ô∏è  {col}: IQR=0 (sem variabilidade), pulando...\")\n",
    "                continue\n",
    "            \n",
    "            lower_bound = Q1 - 3 * IQR\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            \n",
    "            outliers = df_outliers_clean[\n",
    "                (df_outliers_clean[col] < lower_bound) | \n",
    "                (df_outliers_clean[col] > upper_bound)\n",
    "            ]\n",
    "            outliers_dict[col] = outliers\n",
    "            \n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Q1: {Q1:,.2f} | Q3: {Q3:,.2f} | IQR: {IQR:,.2f}\")\n",
    "            print(f\"  Limites: [{lower_bound:,.2f}, {upper_bound:,.2f}]\")\n",
    "            print(f\"  Outliers: {len(outliers):,} ({len(outliers)/len(df_outliers_clean)*100:.2f}%)\")\n",
    "            \n",
    "            if len(outliers) > 0:\n",
    "                print(f\"  Range: {outliers[col].min():,.2f} a {outliers[col].max():,.2f}\")\n",
    "        \n",
    "        # M√©todo 2: Percentis (mais robusto)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DETEC√á√ÉO POR PERCENTIS (P99)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        outliers_percentil = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            p99 = df_outliers_clean[col].quantile(0.99)\n",
    "            p01 = df_outliers_clean[col].quantile(0.01)\n",
    "            \n",
    "            outliers = df_outliers_clean[\n",
    "                (df_outliers_clean[col] > p99) | \n",
    "                (df_outliers_clean[col] < p01)\n",
    "            ]\n",
    "            outliers_percentil[col] = outliers\n",
    "            \n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  P01: {p01:,.2f} | P99: {p99:,.2f}\")\n",
    "            print(f\"  Outliers (fora P01-P99): {len(outliers):,} ({len(outliers)/len(df_outliers_clean)*100:.2f}%)\")\n",
    "        \n",
    "        # Identificar casos extremos (m√∫ltiplos crit√©rios)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CASOS EXTREMOS - M√öLTIPLOS CRIT√âRIOS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Marcar outliers\n",
    "        df_outliers_clean['outlier_valor'] = df_outliers_clean['valor_devido'] > df_outliers_clean['valor_devido'].quantile(0.99)\n",
    "        df_outliers_clean['outlier_tempo'] = df_outliers_clean['dias_cobranca'] > df_outliers_clean['dias_cobranca'].quantile(0.99)\n",
    "        df_outliers_clean['outlier_score'] = df_outliers_clean['score'] > df_outliers_clean['score'].quantile(0.95)\n",
    "        \n",
    "        df_outliers_clean['qtd_outliers'] = (\n",
    "            df_outliers_clean['outlier_valor'].astype(int) +\n",
    "            df_outliers_clean['outlier_tempo'].astype(int) +\n",
    "            df_outliers_clean['outlier_score'].astype(int)\n",
    "        )\n",
    "        \n",
    "        casos_extremos = df_outliers_clean[\n",
    "            df_outliers_clean['qtd_outliers'] >= 2\n",
    "        ].sort_values('valor_devido', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTotal de casos extremos (2+ crit√©rios): {len(casos_extremos):,}\")\n",
    "        print(f\"Percentual do total: {len(casos_extremos)/len(df_outliers_clean)*100:.2f}%\\n\")\n",
    "        \n",
    "        # Top 20 √önicos (sem repeti√ß√£o) - CORRIGIDO\n",
    "        top20_unicos = casos_extremos.drop_duplicates(subset=['inscricao_estadual']).head(20)\n",
    "        \n",
    "        print(f\"Top 20 Empresas com Casos Mais Cr√≠ticos (sem repeti√ß√£o):\\n\")\n",
    "        print(f\"{'#':<3} {'IE':<15} {'Tipo':<10} {'Raz√£o Social':<30} {'Maior Valor':<18} {'D√©bitos':<8} {'Score M√©d':<9}\")\n",
    "        print(\"=\"*110)\n",
    "        \n",
    "        for idx, (i, row) in enumerate(top20_unicos.iterrows(), 1):\n",
    "            razao = str(row['razao_social'])[:28] if pd.notna(row['razao_social']) else 'N/A'\n",
    "            \n",
    "            # CORRIGIDO: tipo_debito pode ser Series, converter para string\n",
    "            tipo_val = row['tipo_debito']\n",
    "            if isinstance(tipo_val, pd.Series):\n",
    "                tipo_val = tipo_val.iloc[0] if len(tipo_val) > 0 else 'N/A'\n",
    "            tipo = str(tipo_val)[:8] if pd.notna(tipo_val) else 'N/A'\n",
    "            \n",
    "            # Contar quantos d√©bitos essa empresa tem\n",
    "            debitos_empresa = casos_extremos[casos_extremos['inscricao_estadual'] == row['inscricao_estadual']]\n",
    "            qtd_debitos = len(debitos_empresa)\n",
    "            score_medio = debitos_empresa['score'].mean()\n",
    "            \n",
    "            print(f\"{idx:<3} {row['inscricao_estadual']:<15} {tipo:<10} {razao:<30} \"\n",
    "                  f\"R$ {row['valor_devido']:>13,.2f} {qtd_debitos:>7} {score_medio:>8.1f}\")\n",
    "        \n",
    "        # Visualiza√ß√£o\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        fig.suptitle('An√°lise de Outliers - Detec√ß√£o de Casos At√≠picos (V1.4)', \n",
    "                     fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Box plots\n",
    "        df_norm = df_outliers_clean[numeric_cols].copy()\n",
    "        for col in numeric_cols:\n",
    "            if df_norm[col].std() > 0:\n",
    "                df_norm[col] = (df_norm[col] - df_norm[col].mean()) / df_norm[col].std()\n",
    "        \n",
    "        bp = axes[0, 0].boxplot([df_norm[col].dropna() for col in numeric_cols],\n",
    "                                labels=['Valor', 'Dias', 'Contatos', 'Score'],\n",
    "                                patch_artist=True, showfliers=True)\n",
    "        \n",
    "        colors_box = ['coral', 'steelblue', 'lightgreen', 'gold']\n",
    "        for patch, color in zip(bp['boxes'], colors_box):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        axes[0, 0].set_title('Box Plots Normalizados')\n",
    "        axes[0, 0].set_ylabel('Z-Score')\n",
    "        axes[0, 0].axhline(y=3, color='red', linestyle='--', alpha=0.5, label='¬±3œÉ')\n",
    "        axes[0, 0].axhline(y=-3, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 2. Histograma log\n",
    "        axes[0, 1].hist(np.log10(df_outliers_clean['valor_devido']+1), \n",
    "                       bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Log10(Valor)')\n",
    "        axes[0, 1].set_ylabel('Frequ√™ncia')\n",
    "        axes[0, 1].set_title('Distribui√ß√£o de Valores (log)')\n",
    "        p99 = df_outliers_clean['valor_devido'].quantile(0.99)\n",
    "        axes[0, 1].axvline(np.log10(p99), color='red', linestyle='--', \n",
    "                          linewidth=2, label=f'P99: R$ {p99/1e6:.1f}M')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Distribui√ß√£o de outliers\n",
    "        outlier_dist = df_outliers_clean['qtd_outliers'].value_counts().sort_index()\n",
    "        colors_sev = ['green', 'yellow', 'orange', 'red']\n",
    "        axes[0, 2].bar(outlier_dist.index, outlier_dist.values,\n",
    "                      color=[colors_sev[int(i)] for i in outlier_dist.index])\n",
    "        axes[0, 2].set_xlabel('N¬∫ de Crit√©rios Outlier')\n",
    "        axes[0, 2].set_ylabel('Quantidade')\n",
    "        axes[0, 2].set_title('Distribui√ß√£o por Severidade')\n",
    "        axes[0, 2].set_xticks(outlier_dist.index)\n",
    "        for i, v in enumerate(outlier_dist.values):\n",
    "            axes[0, 2].text(outlier_dist.index[i], v, f'{v:,}', \n",
    "                           ha='center', va='bottom')\n",
    "        axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 4. Scatter Valor vs Dias - CORRIGIDO com builtins.min\n",
    "        sample_size = builtins.min(10000, len(df_outliers_clean))\n",
    "        sample = df_outliers_clean.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        normal = sample[sample['qtd_outliers'] == 0]\n",
    "        extremos = sample[sample['qtd_outliers'] >= 2]\n",
    "        \n",
    "        axes[1, 0].scatter(normal['dias_cobranca'], normal['valor_devido']/1000,\n",
    "                          alpha=0.3, s=5, c='blue', label='Normal')\n",
    "        axes[1, 0].scatter(extremos['dias_cobranca'], extremos['valor_devido']/1000,\n",
    "                          alpha=0.8, s=40, c='red', marker='^', \n",
    "                          edgecolors='black', linewidth=1, label='Extremos')\n",
    "        axes[1, 0].set_xlabel('Dias em Cobran√ßa')\n",
    "        axes[1, 0].set_ylabel('Valor (Mil R$)')\n",
    "        axes[1, 0].set_title(f'Valor vs Tempo (amostra {sample_size:,})')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Scatter Score vs Valor\n",
    "        axes[1, 1].scatter(normal['score'], normal['valor_devido']/1000,\n",
    "                          alpha=0.3, s=5, c='blue', label='Normal')\n",
    "        axes[1, 1].scatter(extremos['score'], extremos['valor_devido']/1000,\n",
    "                          alpha=0.8, s=40, c='red', marker='^',\n",
    "                          edgecolors='black', linewidth=1, label='Extremos')\n",
    "        axes[1, 1].set_xlabel('Score de Prioriza√ß√£o')\n",
    "        axes[1, 1].set_ylabel('Valor (Mil R$)')\n",
    "        axes[1, 1].set_title('Score vs Valor')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Top 10 maiores valores (empresas √∫nicas)\n",
    "        top10_empresas = top20_unicos.head(10)\n",
    "        axes[1, 2].barh(range(len(top10_empresas)), top10_empresas['valor_devido']/1e6, \n",
    "                       color='darkred', alpha=0.7)\n",
    "        axes[1, 2].set_yticks(range(len(top10_empresas)))\n",
    "        axes[1, 2].set_yticklabels([str(ie)[:12] for ie in top10_empresas['inscricao_estadual']], \n",
    "                                   fontsize=8)\n",
    "        axes[1, 2].set_xlabel('Maior D√©bito (Milh√µes R$)')\n",
    "        axes[1, 2].set_title('Top 10 Empresas - Maior D√©bito')\n",
    "        axes[1, 2].invert_yaxis()\n",
    "        for i, v in enumerate(top10_empresas['valor_devido']/1e6):\n",
    "            axes[1, 2].text(v, i, f' R$ {v:.1f}M', va='center', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Salvar outliers identificados (empresas √∫nicas)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Salvando casos extremos identificados...\")\n",
    "        \n",
    "        casos_unicos = casos_extremos.drop_duplicates(subset=['inscricao_estadual'])\n",
    "        \n",
    "        casos_extremos_spark = spark.createDataFrame(\n",
    "            casos_unicos[['inscricao_estadual', 'tipo_debito', 'valor_devido', 'dias_cobranca', \n",
    "                         'score', 'qtd_outliers']]\n",
    "        )\n",
    "        casos_extremos_spark.write.mode('overwrite').saveAsTable(\n",
    "            'gecob.prior_outliers_identificados')\n",
    "        \n",
    "        print(f\"‚úÖ {len(casos_unicos):,} empresas extremas salvas em: gecob.prior_outliers_identificados\")\n",
    "        \n",
    "        # Resumo\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RESUMO DA AN√ÅLISE DE OUTLIERS:\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"  Total analisado: {len(df_outliers_clean):,} d√©bitos\")\n",
    "        print(f\"  D√©bitos extremos (2+ crit√©rios): {len(casos_extremos):,} ({len(casos_extremos)/len(df_outliers_clean)*100:.2f}%)\")\n",
    "        print(f\"  Empresas √∫nicas com casos extremos: {len(casos_unicos):,}\")\n",
    "        print(f\"  Maior d√©bito: R$ {df_outliers_clean['valor_devido'].max():,.2f}\")\n",
    "        print(f\"  D√©bito mais antigo: {df_outliers_clean['dias_cobranca'].max():.0f} dias\")\n",
    "        print(f\"  Score m√°ximo: {df_outliers_clean['score'].max():.1f}\")\n",
    "        print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c81e8-c308-465a-8b2b-53976c8600a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AN√ÅLISES SQL AVAN√áADAS - CLUSTERS E OUTLIERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AN√ÅLISES SQL AVAN√áADAS - SISTEMA DE PRIORIZA√á√ÉO GECOB/SC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE 1: AN√ÅLISES DA TABELA CLUSTERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1. AN√ÅLISES DOS CLUSTERS IDENTIFICADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1.1 - Estat√≠sticas Globais por Cluster\n",
    "print(\"\\n1.1 - ESTAT√çSTICAS GLOBAIS POR CLUSTER\\n\")\n",
    "\n",
    "query_stats_clusters = \"\"\"\n",
    "SELECT \n",
    "    cluster,\n",
    "    \n",
    "    -- Volumetria\n",
    "    COUNT(DISTINCT inscricao_estadual) as qtd_empresas,\n",
    "    COUNT(*) as qtd_total_registros,\n",
    "    \n",
    "    -- Estat√≠sticas de Valor\n",
    "    SUM(valor_total) as valor_total_cluster,\n",
    "    AVG(valor_total) as valor_medio,\n",
    "    MIN(valor_total) as valor_minimo,\n",
    "    MAX(valor_total) as valor_maximo,\n",
    "    STDDEV(valor_total) as desvio_padrao_valor,\n",
    "    PERCENTILE(valor_total, 0.25) as p25_valor,\n",
    "    PERCENTILE(valor_total, 0.50) as mediana_valor,\n",
    "    PERCENTILE(valor_total, 0.75) as p75_valor,\n",
    "    PERCENTILE(valor_total, 0.95) as p95_valor,\n",
    "    \n",
    "    -- Estat√≠sticas de Tempo\n",
    "    AVG(dias_cobranca) as dias_medio,\n",
    "    MIN(dias_cobranca) as dias_minimo,\n",
    "    MAX(dias_cobranca) as dias_maximo,\n",
    "    STDDEV(dias_cobranca) as desvio_padrao_dias,\n",
    "    PERCENTILE(dias_cobranca, 0.50) as mediana_dias,\n",
    "    \n",
    "    -- Estat√≠sticas de Score\n",
    "    AVG(score_priorizacao) as score_medio,\n",
    "    MIN(score_priorizacao) as score_minimo,\n",
    "    MAX(score_priorizacao) as score_maximo,\n",
    "    STDDEV(score_priorizacao) as desvio_padrao_score,\n",
    "    PERCENTILE(score_priorizacao, 0.50) as mediana_score,\n",
    "    \n",
    "    -- Concentra√ß√£o\n",
    "    SUM(valor_total) / (SELECT SUM(valor_total) FROM gecob.prior_clusters_empresas) * 100 as pct_valor_total,\n",
    "    COUNT(*) / (SELECT COUNT(*) FROM gecob.prior_clusters_empresas) * 100 as pct_registros\n",
    "\n",
    "FROM gecob.prior_clusters_empresas\n",
    "GROUP BY cluster\n",
    "ORDER BY cluster\n",
    "\"\"\"\n",
    "\n",
    "df_stats_clusters = spark.sql(query_stats_clusters).toPandas()\n",
    "\n",
    "# Converter para num√©rico\n",
    "for col in df_stats_clusters.columns:\n",
    "    if col != 'cluster':\n",
    "        df_stats_clusters[col] = pd.to_numeric(df_stats_clusters[col], errors='coerce')\n",
    "\n",
    "print(df_stats_clusters.to_string(index=False))\n",
    "\n",
    "# 1.2 - An√°lise de Concentra√ß√£o (Curva ABC)\n",
    "print(\"\\n1.2 - AN√ÅLISE DE CONCENTRA√á√ÉO (CURVA ABC)\\n\")\n",
    "\n",
    "query_abc = \"\"\"\n",
    "SELECT \n",
    "    cluster,\n",
    "    COUNT(DISTINCT inscricao_estadual) as empresas,\n",
    "    SUM(valor_total) as valor_cluster,\n",
    "    SUM(SUM(valor_total)) OVER (ORDER BY SUM(valor_total) DESC) as valor_acumulado,\n",
    "    SUM(valor_total) / SUM(SUM(valor_total)) OVER () * 100 as pct_valor,\n",
    "    SUM(SUM(valor_total)) OVER (ORDER BY SUM(valor_total) DESC) / \n",
    "        SUM(SUM(valor_total)) OVER () * 100 as pct_acumulado,\n",
    "    CASE \n",
    "        WHEN SUM(SUM(valor_total)) OVER (ORDER BY SUM(valor_total) DESC) / \n",
    "             SUM(SUM(valor_total)) OVER () <= 0.80 THEN 'A - Alta Prioridade'\n",
    "        WHEN SUM(SUM(valor_total)) OVER (ORDER BY SUM(valor_total) DESC) / \n",
    "             SUM(SUM(valor_total)) OVER () <= 0.95 THEN 'B - M√©dia Prioridade'\n",
    "        ELSE 'C - Baixa Prioridade'\n",
    "    END as classe_abc\n",
    "FROM gecob.prior_clusters_empresas\n",
    "GROUP BY cluster\n",
    "ORDER BY valor_cluster DESC\n",
    "\"\"\"\n",
    "\n",
    "df_abc = spark.sql(query_abc).toPandas()\n",
    "for col in df_abc.columns:\n",
    "    if col not in ['cluster', 'classe_abc']:\n",
    "        df_abc[col] = pd.to_numeric(df_abc[col], errors='coerce')\n",
    "\n",
    "print(df_abc.to_string(index=False))\n",
    "\n",
    "# 1.3 - Segmenta√ß√£o por Faixas de Valor\n",
    "print(\"\\n1.3 - SEGMENTA√á√ÉO POR FAIXAS DE VALOR\\n\")\n",
    "\n",
    "query_faixas = \"\"\"\n",
    "SELECT \n",
    "    cluster,\n",
    "    CASE \n",
    "        WHEN valor_total < 1000 THEN '1. At√© R$ 1k'\n",
    "        WHEN valor_total < 10000 THEN '2. R$ 1k a 10k'\n",
    "        WHEN valor_total < 50000 THEN '3. R$ 10k a 50k'\n",
    "        WHEN valor_total < 100000 THEN '4. R$ 50k a 100k'\n",
    "        WHEN valor_total < 500000 THEN '5. R$ 100k a 500k'\n",
    "        WHEN valor_total < 1000000 THEN '6. R$ 500k a 1M'\n",
    "        ELSE '7. Acima de R$ 1M'\n",
    "    END as faixa_valor,\n",
    "    \n",
    "    COUNT(DISTINCT inscricao_estadual) as empresas,\n",
    "    COUNT(*) as debitos,\n",
    "    SUM(valor_total) as valor_total_faixa,\n",
    "    AVG(valor_total) as valor_medio_faixa,\n",
    "    AVG(dias_cobranca) as dias_medio_faixa,\n",
    "    AVG(score_priorizacao) as score_medio_faixa\n",
    "\n",
    "FROM gecob.prior_clusters_empresas\n",
    "GROUP BY cluster, \n",
    "    CASE \n",
    "        WHEN valor_total < 1000 THEN '1. At√© R$ 1k'\n",
    "        WHEN valor_total < 10000 THEN '2. R$ 1k a 10k'\n",
    "        WHEN valor_total < 50000 THEN '3. R$ 10k a 50k'\n",
    "        WHEN valor_total < 100000 THEN '4. R$ 50k a 100k'\n",
    "        WHEN valor_total < 500000 THEN '5. R$ 100k a 500k'\n",
    "        WHEN valor_total < 1000000 THEN '6. R$ 500k a 1M'\n",
    "        ELSE '7. Acima de R$ 1M'\n",
    "    END\n",
    "ORDER BY cluster, faixa_valor\n",
    "\"\"\"\n",
    "\n",
    "df_faixas = spark.sql(query_faixas).toPandas()\n",
    "for col in df_faixas.columns:\n",
    "    if col not in ['cluster', 'faixa_valor']:\n",
    "        df_faixas[col] = pd.to_numeric(df_faixas[col], errors='coerce')\n",
    "\n",
    "print(df_faixas.to_string(index=False))\n",
    "\n",
    "# 1.4 - An√°lise Temporal (Aging)\n",
    "print(\"\\n1.4 - AN√ÅLISE TEMPORAL (AGING DE D√âBITOS)\\n\")\n",
    "\n",
    "query_aging = \"\"\"\n",
    "SELECT \n",
    "    cluster,\n",
    "    CASE \n",
    "        WHEN dias_cobranca <= 30 THEN '1. 0-30 dias'\n",
    "        WHEN dias_cobranca <= 90 THEN '2. 31-90 dias'\n",
    "        WHEN dias_cobranca <= 180 THEN '3. 91-180 dias'\n",
    "        WHEN dias_cobranca <= 365 THEN '4. 181-365 dias'\n",
    "        WHEN dias_cobranca <= 730 THEN '5. 1-2 anos'\n",
    "        ELSE '6. Mais de 2 anos'\n",
    "    END as faixa_aging,\n",
    "    \n",
    "    COUNT(DISTINCT inscricao_estadual) as empresas,\n",
    "    COUNT(*) as debitos,\n",
    "    SUM(valor_total) as valor_total,\n",
    "    AVG(valor_total) as valor_medio,\n",
    "    AVG(score_priorizacao) as score_medio,\n",
    "    \n",
    "    -- Percentual dentro do cluster\n",
    "    COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY cluster) as pct_debitos_cluster\n",
    "\n",
    "FROM gecob.prior_clusters_empresas\n",
    "GROUP BY cluster,\n",
    "    CASE \n",
    "        WHEN dias_cobranca <= 30 THEN '1. 0-30 dias'\n",
    "        WHEN dias_cobranca <= 90 THEN '2. 31-90 dias'\n",
    "        WHEN dias_cobranca <= 180 THEN '3. 91-180 dias'\n",
    "        WHEN dias_cobranca <= 365 THEN '4. 181-365 dias'\n",
    "        WHEN dias_cobranca <= 730 THEN '5. 1-2 anos'\n",
    "        ELSE '6. Mais de 2 anos'\n",
    "    END\n",
    "ORDER BY cluster, faixa_aging\n",
    "\"\"\"\n",
    "\n",
    "df_aging = spark.sql(query_aging).toPandas()\n",
    "for col in df_aging.columns:\n",
    "    if col not in ['cluster', 'faixa_aging']:\n",
    "        df_aging[col] = pd.to_numeric(df_aging[col], errors='coerce')\n",
    "\n",
    "print(df_aging.to_string(index=False))\n",
    "\n",
    "# 1.5 - Top Empresas por Cluster\n",
    "print(\"\\n1.5 - TOP 10 EMPRESAS POR CLUSTER (MAIOR VALOR TOTAL)\\n\")\n",
    "\n",
    "query_top_clusters = \"\"\"\n",
    "WITH empresas_totais AS (\n",
    "    SELECT \n",
    "        cluster,\n",
    "        inscricao_estadual,\n",
    "        COUNT(*) as qtd_debitos,\n",
    "        SUM(valor_total) as valor_total_empresa,\n",
    "        AVG(dias_cobranca) as dias_medio,\n",
    "        AVG(score_priorizacao) as score_medio,\n",
    "        MIN(valor_total) as menor_debito,\n",
    "        MAX(valor_total) as maior_debito\n",
    "    FROM gecob.prior_clusters_empresas\n",
    "    GROUP BY cluster, inscricao_estadual\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY cluster ORDER BY valor_total_empresa DESC) as rank\n",
    "    FROM empresas_totais\n",
    ")\n",
    "SELECT \n",
    "    cluster,\n",
    "    rank,\n",
    "    inscricao_estadual,\n",
    "    qtd_debitos,\n",
    "    valor_total_empresa,\n",
    "    dias_medio,\n",
    "    score_medio,\n",
    "    menor_debito,\n",
    "    maior_debito\n",
    "FROM ranked\n",
    "WHERE rank <= 10\n",
    "ORDER BY cluster, rank\n",
    "\"\"\"\n",
    "\n",
    "df_top_clusters = spark.sql(query_top_clusters).toPandas()\n",
    "for col in df_top_clusters.columns:\n",
    "    if col not in ['cluster', 'inscricao_estadual']:\n",
    "        df_top_clusters[col] = pd.to_numeric(df_top_clusters[col], errors='coerce')\n",
    "\n",
    "print(df_top_clusters.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE 2: AN√ÅLISES DA TABELA OUTLIERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. AN√ÅLISES DOS OUTLIERS IDENTIFICADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2.1 - Estat√≠sticas Globais de Outliers\n",
    "print(\"\\n2.1 - ESTAT√çSTICAS GLOBAIS DOS OUTLIERS\\n\")\n",
    "\n",
    "query_stats_outliers = \"\"\"\n",
    "SELECT \n",
    "    -- Severidade\n",
    "    qtd_outliers as nivel_severidade,\n",
    "    \n",
    "    -- Volumetria\n",
    "    COUNT(DISTINCT inscricao_estadual) as empresas,\n",
    "    \n",
    "    -- Estat√≠sticas de Valor\n",
    "    SUM(valor_devido) as valor_total,\n",
    "    AVG(valor_devido) as valor_medio,\n",
    "    MIN(valor_devido) as valor_minimo,\n",
    "    MAX(valor_devido) as valor_maximo,\n",
    "    STDDEV(valor_devido) as desvio_padrao_valor,\n",
    "    PERCENTILE(valor_devido, 0.50) as mediana_valor,\n",
    "    PERCENTILE(valor_devido, 0.95) as p95_valor,\n",
    "    \n",
    "    -- Estat√≠sticas de Tempo\n",
    "    AVG(dias_cobranca) as dias_medio,\n",
    "    MIN(dias_cobranca) as dias_minimo,\n",
    "    MAX(dias_cobranca) as dias_maximo,\n",
    "    PERCENTILE(dias_cobranca, 0.50) as mediana_dias,\n",
    "    \n",
    "    -- Estat√≠sticas de Score\n",
    "    AVG(score) as score_medio,\n",
    "    MIN(score) as score_minimo,\n",
    "    MAX(score) as score_maximo,\n",
    "    PERCENTILE(score, 0.50) as mediana_score,\n",
    "    \n",
    "    -- Concentra√ß√£o\n",
    "    SUM(valor_devido) / (SELECT SUM(valor_devido) FROM gecob.prior_outliers_identificados) * 100 as pct_valor_total\n",
    "\n",
    "FROM gecob.prior_outliers_identificados\n",
    "GROUP BY qtd_outliers\n",
    "ORDER BY qtd_outliers DESC\n",
    "\"\"\"\n",
    "\n",
    "df_stats_outliers = spark.sql(query_stats_outliers).toPandas()\n",
    "for col in df_stats_outliers.columns:\n",
    "    if col != 'nivel_severidade':\n",
    "        df_stats_outliers[col] = pd.to_numeric(df_stats_outliers[col], errors='coerce')\n",
    "\n",
    "print(df_stats_outliers.to_string(index=False))\n",
    "\n",
    "# 2.2 - Top 20 Outliers Mais Cr√≠ticos\n",
    "print(\"\\n2.2 - TOP 20 OUTLIERS MAIS CR√çTICOS\\n\")\n",
    "\n",
    "query_top_outliers = \"\"\"\n",
    "SELECT \n",
    "    ROW_NUMBER() OVER (ORDER BY valor_devido DESC) as rank,\n",
    "    inscricao_estadual,\n",
    "    valor_devido,\n",
    "    dias_cobranca,\n",
    "    score,\n",
    "    qtd_outliers as severidade,\n",
    "    \n",
    "    -- Classifica√ß√£o de risco\n",
    "    CASE \n",
    "        WHEN valor_devido > 100000000 THEN 'CR√çTICO - Acima de R$ 100M'\n",
    "        WHEN valor_devido > 50000000 THEN 'MUITO ALTO - R$ 50M a 100M'\n",
    "        WHEN valor_devido > 10000000 THEN 'ALTO - R$ 10M a 50M'\n",
    "        WHEN valor_devido > 1000000 THEN 'M√âDIO-ALTO - R$ 1M a 10M'\n",
    "        ELSE 'M√âDIO - At√© R$ 1M'\n",
    "    END as classificacao_risco,\n",
    "    \n",
    "    -- Urg√™ncia temporal\n",
    "    CASE \n",
    "        WHEN dias_cobranca > 730 THEN 'URGENTE - Mais de 2 anos'\n",
    "        WHEN dias_cobranca > 365 THEN 'ALTA - 1 a 2 anos'\n",
    "        WHEN dias_cobranca > 180 THEN 'M√âDIA - 6 meses a 1 ano'\n",
    "        ELSE 'RECENTE - At√© 6 meses'\n",
    "    END as urgencia_temporal\n",
    "\n",
    "FROM gecob.prior_outliers_identificados\n",
    "ORDER BY valor_devido DESC\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "df_top_outliers = spark.sql(query_top_outliers).toPandas()\n",
    "for col in df_top_outliers.columns:\n",
    "    if col not in ['inscricao_estadual', 'classificacao_risco', 'urgencia_temporal']:\n",
    "        df_top_outliers[col] = pd.to_numeric(df_top_outliers[col], errors='coerce')\n",
    "\n",
    "print(df_top_outliers.to_string(index=False))\n",
    "\n",
    "# 2.3 - Segmenta√ß√£o de Outliers\n",
    "print(\"\\n2.3 - SEGMENTA√á√ÉO DOS OUTLIERS POR VALOR E TEMPO\\n\")\n",
    "\n",
    "query_seg_outliers = \"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN valor_devido > 100000000 THEN '1. > R$ 100M'\n",
    "        WHEN valor_devido > 50000000 THEN '2. R$ 50M - 100M'\n",
    "        WHEN valor_devido > 10000000 THEN '3. R$ 10M - 50M'\n",
    "        WHEN valor_devido > 1000000 THEN '4. R$ 1M - 10M'\n",
    "        ELSE '5. < R$ 1M'\n",
    "    END as faixa_valor,\n",
    "    \n",
    "    CASE \n",
    "        WHEN dias_cobranca > 730 THEN 'D. > 2 anos'\n",
    "        WHEN dias_cobranca > 365 THEN 'C. 1-2 anos'\n",
    "        WHEN dias_cobranca > 180 THEN 'B. 6m-1ano'\n",
    "        ELSE 'A. < 6 meses'\n",
    "    END as faixa_tempo,\n",
    "    \n",
    "    COUNT(DISTINCT inscricao_estadual) as empresas,\n",
    "    SUM(valor_devido) as valor_total,\n",
    "    AVG(valor_devido) as valor_medio,\n",
    "    AVG(score) as score_medio,\n",
    "    AVG(qtd_outliers) as severidade_media\n",
    "\n",
    "FROM gecob.prior_outliers_identificados\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN valor_devido > 100000000 THEN '1. > R$ 100M'\n",
    "        WHEN valor_devido > 50000000 THEN '2. R$ 50M - 100M'\n",
    "        WHEN valor_devido > 10000000 THEN '3. R$ 10M - 50M'\n",
    "        WHEN valor_devido > 1000000 THEN '4. R$ 1M - 10M'\n",
    "        ELSE '5. < R$ 1M'\n",
    "    END,\n",
    "    CASE \n",
    "        WHEN dias_cobranca > 730 THEN 'D. > 2 anos'\n",
    "        WHEN dias_cobranca > 365 THEN 'C. 1-2 anos'\n",
    "        WHEN dias_cobranca > 180 THEN 'B. 6m-1ano'\n",
    "        ELSE 'A. < 6 meses'\n",
    "    END\n",
    "ORDER BY faixa_valor, faixa_tempo\n",
    "\"\"\"\n",
    "\n",
    "df_seg_outliers = spark.sql(query_seg_outliers).toPandas()\n",
    "for col in df_seg_outliers.columns:\n",
    "    if col not in ['faixa_valor', 'faixa_tempo']:\n",
    "        df_seg_outliers[col] = pd.to_numeric(df_seg_outliers[col], errors='coerce')\n",
    "\n",
    "print(df_seg_outliers.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE 3: AN√ÅLISE CRUZADA CLUSTERS vs OUTLIERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. AN√ÅLISE CRUZADA: CLUSTERS vs OUTLIERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 3.1 - Outliers por Cluster\n",
    "print(\"\\n3.1 - DISTRIBUI√á√ÉO DE OUTLIERS POR CLUSTER\\n\")\n",
    "\n",
    "query_cross = \"\"\"\n",
    "SELECT \n",
    "    c.cluster,\n",
    "    COUNT(DISTINCT c.inscricao_estadual) as total_empresas_cluster,\n",
    "    COUNT(DISTINCT o.inscricao_estadual) as empresas_outliers,\n",
    "    COUNT(DISTINCT o.inscricao_estadual) * 100.0 / COUNT(DISTINCT c.inscricao_estadual) as pct_outliers,\n",
    "    \n",
    "    SUM(c.valor_total) as valor_total_cluster,\n",
    "    SUM(CASE WHEN o.inscricao_estadual IS NOT NULL THEN c.valor_total ELSE 0 END) as valor_outliers,\n",
    "    SUM(CASE WHEN o.inscricao_estadual IS NOT NULL THEN c.valor_total ELSE 0 END) * 100.0 / \n",
    "        SUM(c.valor_total) as pct_valor_outliers,\n",
    "    \n",
    "    AVG(CASE WHEN o.inscricao_estadual IS NOT NULL THEN c.score_priorizacao END) as score_medio_outliers,\n",
    "    AVG(CASE WHEN o.inscricao_estadual IS NULL THEN c.score_priorizacao END) as score_medio_normais,\n",
    "    \n",
    "    AVG(o.qtd_outliers) as severidade_media\n",
    "\n",
    "FROM gecob.prior_clusters_empresas c\n",
    "LEFT JOIN gecob.prior_outliers_identificados o \n",
    "    ON c.inscricao_estadual = o.inscricao_estadual\n",
    "GROUP BY c.cluster\n",
    "ORDER BY c.cluster\n",
    "\"\"\"\n",
    "\n",
    "df_cross = spark.sql(query_cross).toPandas()\n",
    "for col in df_cross.columns:\n",
    "    if col != 'cluster':\n",
    "        df_cross[col] = pd.to_numeric(df_cross[col], errors='coerce')\n",
    "\n",
    "print(df_cross.to_string(index=False))\n",
    "\n",
    "# 3.2 - Matriz de Risco (Valor x Tempo)\n",
    "print(\"\\n3.2 - MATRIZ DE RISCO: VALOR vs ANTIGUIDADE\\n\")\n",
    "\n",
    "query_matriz = \"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN c.valor_total > 100000 THEN 'ALTO VALOR'\n",
    "        WHEN c.valor_total > 10000 THEN 'M√âDIO VALOR'\n",
    "        ELSE 'BAIXO VALOR'\n",
    "    END as categoria_valor,\n",
    "    \n",
    "    CASE \n",
    "        WHEN c.dias_cobranca > 365 THEN 'ANTIGO'\n",
    "        ELSE 'RECENTE'\n",
    "    END as categoria_tempo,\n",
    "    \n",
    "    c.cluster,\n",
    "    \n",
    "    COUNT(DISTINCT c.inscricao_estadual) as empresas,\n",
    "    COUNT(DISTINCT o.inscricao_estadual) as empresas_outliers,\n",
    "    SUM(c.valor_total) as valor_total,\n",
    "    AVG(c.score_priorizacao) as score_medio,\n",
    "    \n",
    "    -- Recomenda√ß√£o de a√ß√£o\n",
    "    CASE \n",
    "        WHEN c.valor_total > 100000 AND c.dias_cobranca > 365 THEN 'üî¥ A√á√ÉO IMEDIATA'\n",
    "        WHEN c.valor_total > 100000 OR c.dias_cobranca > 365 THEN 'üü° A√á√ÉO PRIORIT√ÅRIA'\n",
    "        ELSE 'üü¢ MONITORAMENTO'\n",
    "    END as recomendacao\n",
    "\n",
    "FROM gecob.prior_clusters_empresas c\n",
    "LEFT JOIN gecob.prior_outliers_identificados o \n",
    "    ON c.inscricao_estadual = o.inscricao_estadual\n",
    "GROUP BY \n",
    "    CASE \n",
    "        WHEN c.valor_total > 100000 THEN 'ALTO VALOR'\n",
    "        WHEN c.valor_total > 10000 THEN 'M√âDIO VALOR'\n",
    "        ELSE 'BAIXO VALOR'\n",
    "    END,\n",
    "    CASE \n",
    "        WHEN c.dias_cobranca > 365 THEN 'ANTIGO'\n",
    "        ELSE 'RECENTE'\n",
    "    END,\n",
    "    c.cluster,\n",
    "    CASE \n",
    "        WHEN c.valor_total > 100000 AND c.dias_cobranca > 365 THEN 'üî¥ A√á√ÉO IMEDIATA'\n",
    "        WHEN c.valor_total > 100000 OR c.dias_cobranca > 365 THEN 'üü° A√á√ÉO PRIORIT√ÅRIA'\n",
    "        ELSE 'üü¢ MONITORAMENTO'\n",
    "    END\n",
    "ORDER BY categoria_valor DESC, categoria_tempo DESC, cluster\n",
    "\"\"\"\n",
    "\n",
    "df_matriz = spark.sql(query_matriz).toPandas()\n",
    "for col in df_matriz.columns:\n",
    "    if col not in ['categoria_valor', 'categoria_tempo', 'cluster', 'recomendacao']:\n",
    "        df_matriz[col] = pd.to_numeric(df_matriz[col], errors='coerce')\n",
    "\n",
    "print(df_matriz.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b3093-198f-4f05-ad14-cd4c13a48a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins  # Adicionar no topo\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE 4: AN√ÅLISES DE CORRELA√á√ÉO E PADR√ïES (CORRIGIDA)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. AN√ÅLISES DE CORRELA√á√ÉO E PADR√ïES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 4.1 - Correla√ß√£o entre M√©tricas por Cluster\n",
    "print(\"\\n4.1 - AN√ÅLISE DE CORRELA√á√ÉO: VALOR vs TEMPO vs SCORE\\n\")\n",
    "\n",
    "query_correlacao = \"\"\"\n",
    "SELECT \n",
    "    cluster,\n",
    "    \n",
    "    -- Correla√ß√£o Valor x Dias\n",
    "    CORR(valor_total, dias_cobranca) as corr_valor_dias,\n",
    "    \n",
    "    -- Correla√ß√£o Valor x Score\n",
    "    CORR(valor_total, score_priorizacao) as corr_valor_score,\n",
    "    \n",
    "    -- Correla√ß√£o Dias x Score\n",
    "    CORR(dias_cobranca, score_priorizacao) as corr_dias_score,\n",
    "    \n",
    "    -- Coeficiente de varia√ß√£o\n",
    "    STDDEV(valor_total) / AVG(valor_total) as cv_valor,\n",
    "    STDDEV(dias_cobranca) / AVG(dias_cobranca) as cv_dias,\n",
    "    STDDEV(score_priorizacao) / AVG(score_priorizacao) as cv_score,\n",
    "    \n",
    "    -- M√©tricas b√°sicas\n",
    "    COUNT(DISTINCT inscricao_estadual) as empresas,\n",
    "    AVG(valor_total) as valor_medio,\n",
    "    AVG(dias_cobranca) as dias_medio,\n",
    "    AVG(score_priorizacao) as score_medio\n",
    "\n",
    "FROM gecob.prior_clusters_empresas\n",
    "GROUP BY cluster\n",
    "ORDER BY cluster\n",
    "\"\"\"\n",
    "\n",
    "df_correlacao = spark.sql(query_correlacao).toPandas()\n",
    "for col in df_correlacao.columns:\n",
    "    if col != 'cluster':\n",
    "        df_correlacao[col] = pd.to_numeric(df_correlacao[col], errors='coerce')\n",
    "\n",
    "print(df_correlacao.to_string(index=False))\n",
    "\n",
    "print(\"\\nINTERPRETA√á√ÉO:\")\n",
    "for idx, row in df_correlacao.iterrows():\n",
    "    print(f\"\\nCluster {int(row['cluster'])}:\")\n",
    "    \n",
    "    # Valor x Dias - CORRIGIDO\n",
    "    if builtins.abs(row['corr_valor_dias']) > 0.7:\n",
    "        sinal = \"positiva forte\" if row['corr_valor_dias'] > 0 else \"negativa forte\"\n",
    "        print(f\"  ‚Ä¢ Valor vs Dias: Correla√ß√£o {sinal} ({row['corr_valor_dias']:.2f})\")\n",
    "    elif builtins.abs(row['corr_valor_dias']) > 0.3:\n",
    "        sinal = \"positiva moderada\" if row['corr_valor_dias'] > 0 else \"negativa moderada\"\n",
    "        print(f\"  ‚Ä¢ Valor vs Dias: Correla√ß√£o {sinal} ({row['corr_valor_dias']:.2f})\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ Valor vs Dias: Correla√ß√£o fraca ({row['corr_valor_dias']:.2f})\")\n",
    "    \n",
    "    # Variabilidade\n",
    "    if row['cv_valor'] > 1:\n",
    "        print(f\"  ‚Ä¢ Alta variabilidade de valores (CV={row['cv_valor']:.2f})\")\n",
    "    \n",
    "    if row['score_medio'] > 55:\n",
    "        print(f\"  ‚Ä¢ Score alto: {row['score_medio']:.1f} - Prioridade elevada\")\n",
    "\n",
    "# 4.2 - An√°lise de Padr√µes Temporais - CORRIGIDA\n",
    "print(\"\\n\\n4.2 - PADR√ïES TEMPORAIS: EVOLU√á√ÉO POR TRIMESTRE\\n\")\n",
    "\n",
    "query_temporal = \"\"\"\n",
    "WITH periodos_numericos AS (\n",
    "    SELECT \n",
    "        cluster,\n",
    "        CASE \n",
    "            WHEN dias_cobranca <= 90 THEN 1\n",
    "            WHEN dias_cobranca <= 180 THEN 2\n",
    "            WHEN dias_cobranca <= 365 THEN 3\n",
    "            WHEN dias_cobranca <= 730 THEN 4\n",
    "            ELSE 5\n",
    "        END as periodo_num,\n",
    "        CASE \n",
    "            WHEN dias_cobranca <= 90 THEN 'Q1 - 0-90 dias'\n",
    "            WHEN dias_cobranca <= 180 THEN 'Q2 - 91-180 dias'\n",
    "            WHEN dias_cobranca <= 365 THEN 'Q3 - 181-365 dias'\n",
    "            WHEN dias_cobranca <= 730 THEN 'Q4 - 1-2 anos'\n",
    "            ELSE 'Q5 - Mais de 2 anos'\n",
    "        END as periodo,\n",
    "        inscricao_estadual,\n",
    "        valor_total,\n",
    "        score_priorizacao\n",
    "    FROM gecob.prior_clusters_empresas\n",
    "),\n",
    "agregados AS (\n",
    "    SELECT \n",
    "        cluster,\n",
    "        periodo_num,\n",
    "        periodo,\n",
    "        COUNT(DISTINCT inscricao_estadual) as empresas,\n",
    "        SUM(valor_total) as valor_total,\n",
    "        AVG(valor_total) as valor_medio,\n",
    "        AVG(score_priorizacao) as score_medio\n",
    "    FROM periodos_numericos\n",
    "    GROUP BY cluster, periodo_num, periodo\n",
    ")\n",
    "SELECT \n",
    "    cluster,\n",
    "    periodo,\n",
    "    empresas,\n",
    "    valor_total,\n",
    "    valor_medio,\n",
    "    score_medio,\n",
    "    \n",
    "    -- Percentual dentro do cluster\n",
    "    valor_total / SUM(valor_total) OVER (PARTITION BY cluster) * 100 as pct_valor_periodo,\n",
    "    \n",
    "    -- Acumulado\n",
    "    SUM(valor_total) OVER (PARTITION BY cluster ORDER BY periodo_num) as valor_acumulado\n",
    "    \n",
    "FROM agregados\n",
    "ORDER BY cluster, periodo_num\n",
    "\"\"\"\n",
    "\n",
    "df_temporal = spark.sql(query_temporal).toPandas()\n",
    "for col in df_temporal.columns:\n",
    "    if col not in ['cluster', 'periodo']:\n",
    "        df_temporal[col] = pd.to_numeric(df_temporal[col], errors='coerce')\n",
    "\n",
    "print(df_temporal.to_string(index=False))\n",
    "\n",
    "# An√°lise dos padr√µes temporais\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INSIGHTS TEMPORAIS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster in df_temporal['cluster'].unique():\n",
    "    cluster_data = df_temporal[df_temporal['cluster'] == cluster]\n",
    "    \n",
    "    print(f\"\\nCluster {int(cluster)}:\")\n",
    "    \n",
    "    # Per√≠odo com mais valor\n",
    "    periodo_max = cluster_data.loc[cluster_data['valor_total'].idxmax()]\n",
    "    print(f\"  ‚Ä¢ Maior concentra√ß√£o: {periodo_max['periodo']}\")\n",
    "    print(f\"    - R$ {periodo_max['valor_total']/1e6:.2f} milh√µes ({periodo_max['pct_valor_periodo']:.1f}%)\")\n",
    "    \n",
    "    # Taxa de acumula√ß√£o\n",
    "    ultimos_90 = cluster_data[cluster_data['periodo'].str.contains('0-90')]['valor_total'].sum()\n",
    "    total = cluster_data['valor_total'].sum()\n",
    "    if total > 0:\n",
    "        pct_recente = ultimos_90 / total * 100\n",
    "        print(f\"  ‚Ä¢ D√©bitos recentes (0-90 dias): {pct_recente:.1f}% do valor\")\n",
    "        \n",
    "        if pct_recente < 20:\n",
    "            print(f\"    ‚ö†Ô∏è  Baixa entrada de novos d√©bitos - cluster estagnado\")\n",
    "        elif pct_recente > 50:\n",
    "            print(f\"    üìà Alto volume de d√©bitos recentes - crescimento ativo\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE 5: IDENTIFICA√á√ÉO DE CASOS PRIORIT√ÅRIOS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. IDENTIFICA√á√ÉO DE CASOS PRIORIT√ÅRIOS PARA A√á√ÉO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 5.1 - Score de Prioridade Combinado\n",
    "print(\"\\n5.1 - RANKING DE PRIORIDADE COMBINADA (TOP 50)\\n\")\n",
    "\n",
    "query_prioridade = \"\"\"\n",
    "WITH empresas_agregadas AS (\n",
    "    SELECT \n",
    "        c.inscricao_estadual,\n",
    "        c.cluster,\n",
    "        COUNT(*) as qtd_debitos,\n",
    "        SUM(c.valor_total) as valor_total_empresa,\n",
    "        AVG(c.dias_cobranca) as dias_medio,\n",
    "        AVG(c.score_priorizacao) as score_medio,\n",
    "        MAX(c.valor_total) as maior_debito,\n",
    "        \n",
    "        -- Flag outlier\n",
    "        CASE WHEN o.inscricao_estadual IS NOT NULL THEN 1 ELSE 0 END as is_outlier,\n",
    "        COALESCE(o.qtd_outliers, 0) as severidade_outlier,\n",
    "        \n",
    "        -- Score combinado (ponderado)\n",
    "        (AVG(c.score_priorizacao) * 0.4 +\n",
    "         LEAST(SUM(c.valor_total) / 1000000, 100) * 0.3 +\n",
    "         LEAST(AVG(c.dias_cobranca) / 365, 5) * 10 * 0.2 +\n",
    "         CASE WHEN o.inscricao_estadual IS NOT NULL THEN 10 ELSE 0 END * 0.1\n",
    "        ) as score_combinado\n",
    "        \n",
    "    FROM gecob.prior_clusters_empresas c\n",
    "    LEFT JOIN gecob.prior_outliers_identificados o \n",
    "        ON c.inscricao_estadual = o.inscricao_estadual\n",
    "    GROUP BY c.inscricao_estadual, c.cluster, o.inscricao_estadual, o.qtd_outliers\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        ROW_NUMBER() OVER (ORDER BY score_combinado DESC) as ranking_geral,\n",
    "        ROW_NUMBER() OVER (PARTITION BY cluster ORDER BY score_combinado DESC) as ranking_cluster,\n",
    "        \n",
    "        -- Classifica√ß√£o de a√ß√£o\n",
    "        CASE \n",
    "            WHEN score_combinado >= 60 THEN 'üî¥ CR√çTICO - A√ß√£o Imediata'\n",
    "            WHEN score_combinado >= 50 THEN 'üü† ALTO - A√ß√£o em 7 dias'\n",
    "            WHEN score_combinado >= 40 THEN 'üü° M√âDIO - A√ß√£o em 30 dias'\n",
    "            ELSE 'üü¢ BAIXO - Monitoramento'\n",
    "        END as classificacao_acao\n",
    "        \n",
    "    FROM empresas_agregadas\n",
    ")\n",
    "SELECT \n",
    "    ranking_geral,\n",
    "    inscricao_estadual,\n",
    "    cluster,\n",
    "    qtd_debitos,\n",
    "    valor_total_empresa,\n",
    "    dias_medio,\n",
    "    score_medio,\n",
    "    is_outlier,\n",
    "    ROUND(score_combinado, 2) as score_combinado,\n",
    "    classificacao_acao\n",
    "FROM ranked\n",
    "WHERE ranking_geral <= 50\n",
    "ORDER BY ranking_geral\n",
    "\"\"\"\n",
    "\n",
    "df_prioridade = spark.sql(query_prioridade).toPandas()\n",
    "for col in df_prioridade.columns:\n",
    "    if col not in ['inscricao_estadual', 'cluster', 'classificacao_acao']:\n",
    "        df_prioridade[col] = pd.to_numeric(df_prioridade[col], errors='coerce')\n",
    "\n",
    "print(df_prioridade.to_string(index=False))\n",
    "\n",
    "# Salvar lista de prioridades\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Salvando lista de prioridades para a√ß√£o...\")\n",
    "\n",
    "# Criar tabela de a√ß√µes priorit√°rias\n",
    "df_prioridade_spark = spark.createDataFrame(df_prioridade)\n",
    "df_prioridade_spark.write.mode('overwrite').saveAsTable('gecob.prior_acoes_prioritarias')\n",
    "\n",
    "print(f\"‚úì Top 50 casos priorit√°rios salvos em: gecob.prior_acoes_prioritarias\")\n",
    "\n",
    "# 5.2 - Plano de A√ß√£o por Cluster\n",
    "print(\"\\n\\n5.2 - PLANO DE A√á√ÉO RECOMENDADO POR CLUSTER\\n\")\n",
    "\n",
    "query_plano_acao = \"\"\"\n",
    "SELECT \n",
    "    c.cluster,\n",
    "    \n",
    "    -- Volumetria\n",
    "    COUNT(DISTINCT c.inscricao_estadual) as total_empresas,\n",
    "    \n",
    "    -- Por n√≠vel de urg√™ncia\n",
    "    COUNT(DISTINCT CASE WHEN c.score_priorizacao >= 60 THEN c.inscricao_estadual END) as criticos,\n",
    "    COUNT(DISTINCT CASE WHEN c.score_priorizacao >= 50 AND c.score_priorizacao < 60 THEN c.inscricao_estadual END) as altos,\n",
    "    COUNT(DISTINCT CASE WHEN c.score_priorizacao >= 40 AND c.score_priorizacao < 50 THEN c.inscricao_estadual END) as medios,\n",
    "    COUNT(DISTINCT CASE WHEN c.score_priorizacao < 40 THEN c.inscricao_estadual END) as baixos,\n",
    "    \n",
    "    -- Valores\n",
    "    SUM(CASE WHEN c.score_priorizacao >= 60 THEN c.valor_total ELSE 0 END) as valor_criticos,\n",
    "    SUM(CASE WHEN c.score_priorizacao >= 50 THEN c.valor_total ELSE 0 END) as valor_prioritarios,\n",
    "    SUM(c.valor_total) as valor_total,\n",
    "    \n",
    "    -- Outliers\n",
    "    COUNT(DISTINCT o.inscricao_estadual) as qtd_outliers,\n",
    "    \n",
    "    -- M√©tricas m√©dias\n",
    "    AVG(c.valor_total) as ticket_medio,\n",
    "    AVG(c.dias_cobranca) as dias_medio,\n",
    "    \n",
    "    -- Capacidade estimada de cobran√ßa (empresas por m√™s)\n",
    "    CASE \n",
    "        WHEN COUNT(DISTINCT c.inscricao_estadual) > 10000 THEN ROUND(COUNT(DISTINCT c.inscricao_estadual) / 12, 0)\n",
    "        WHEN COUNT(DISTINCT c.inscricao_estadual) > 1000 THEN ROUND(COUNT(DISTINCT c.inscricao_estadual) / 6, 0)\n",
    "        ELSE ROUND(COUNT(DISTINCT c.inscricao_estadual) / 3, 0)\n",
    "    END as capacidade_mensal_estimada\n",
    "\n",
    "FROM gecob.prior_clusters_empresas c\n",
    "LEFT JOIN gecob.prior_outliers_identificados o \n",
    "    ON c.inscricao_estadual = o.inscricao_estadual\n",
    "GROUP BY c.cluster\n",
    "ORDER BY valor_prioritarios DESC\n",
    "\"\"\"\n",
    "\n",
    "df_plano = spark.sql(query_plano_acao).toPandas()\n",
    "for col in df_plano.columns:\n",
    "    if col != 'cluster':\n",
    "        df_plano[col] = pd.to_numeric(df_plano[col], errors='coerce')\n",
    "\n",
    "print(df_plano.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMENDA√á√ïES DE A√á√ÉO POR CLUSTER:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, row in df_plano.iterrows():\n",
    "    cluster = int(row['cluster'])\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CLUSTER {cluster}:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Popula√ß√£o: {int(row['total_empresas']):,} empresas\")\n",
    "    print(f\"  Valor Total: R$ {row['valor_total']/1e6:.2f} milh√µes\")\n",
    "    print(f\"  Ticket M√©dio: R$ {row['ticket_medio']:,.2f}\")\n",
    "    print(f\"  Idade M√©dia: {row['dias_medio']:.0f} dias\")\n",
    "    \n",
    "    print(f\"\\n  ESTRATIFICA√á√ÉO POR PRIORIDADE:\")\n",
    "    total_empresas = row['total_empresas']\n",
    "    if total_empresas > 0:\n",
    "        print(f\"    üî¥ Cr√≠ticos:  {int(row['criticos']):>6,} empresas ({row['criticos']/total_empresas*100:>5.1f}%) - R$ {row['valor_criticos']/1e6:>8.2f}M\")\n",
    "        print(f\"    üü† Altos:     {int(row['altos']):>6,} empresas ({row['altos']/total_empresas*100:>5.1f}%)\")\n",
    "        print(f\"    üü° M√©dios:    {int(row['medios']):>6,} empresas ({row['medios']/total_empresas*100:>5.1f}%)\")\n",
    "        print(f\"    üü¢ Baixos:    {int(row['baixos']):>6,} empresas ({row['baixos']/total_empresas*100:>5.1f}%)\")\n",
    "    \n",
    "    if row['qtd_outliers'] > 0:\n",
    "        print(f\"\\n  ‚ö†Ô∏è  ATEN√á√ÉO: {int(row['qtd_outliers']):,} casos outliers detectados neste cluster\")\n",
    "    \n",
    "    print(f\"\\n  üìã PLANO DE A√á√ÉO SUGERIDO:\")\n",
    "    if row['criticos'] > 0:\n",
    "        dias_criticos = 7\n",
    "        casos_por_dia = row['criticos'] / dias_criticos\n",
    "        print(f\"    ‚ñ∏ Fase 1: Priorizar {int(row['criticos']):,} casos cr√≠ticos (pr√≥ximos {dias_criticos} dias)\")\n",
    "        print(f\"             Ritmo: ~{casos_por_dia:.0f} casos/dia\")\n",
    "        print(f\"             Impacto: R$ {row['valor_criticos']/1e6:.2f} milh√µes\")\n",
    "    if row['altos'] > 0:\n",
    "        print(f\"    ‚ñ∏ Fase 2: Trabalhar {int(row['altos']):,} casos altos (pr√≥ximos 30 dias)\")\n",
    "    if row['medios'] > 0:\n",
    "        print(f\"    ‚ñ∏ Fase 3: Abordar {int(row['medios']):,} casos m√©dios (pr√≥ximos 90 dias)\")\n",
    "    \n",
    "    print(f\"\\n    ‚ñ∏ Capacidade sugerida: {int(row['capacidade_mensal_estimada']):,} casos/m√™s\")\n",
    "    \n",
    "    # Recomenda√ß√£o estrat√©gica\n",
    "    pct_criticos = row['valor_criticos'] / row['valor_total'] if row['valor_total'] > 0 else 0\n",
    "    \n",
    "    print(f\"\\n  üí° ESTRAT√âGIA RECOMENDADA:\")\n",
    "    if pct_criticos > 0.5:\n",
    "        print(f\"     ‚Ä¢ Cluster CONCENTRADO ({pct_criticos*100:.1f}% do valor em cr√≠ticos)\")\n",
    "        print(f\"     ‚Ä¢ Foco: Poucos casos de alto impacto\")\n",
    "        print(f\"     ‚Ä¢ Abordagem: Negocia√ß√£o personalizada, equipe s√™nior\")\n",
    "        print(f\"     ‚Ä¢ KPI: Taxa de acordo em casos cr√≠ticos\")\n",
    "    elif row['total_empresas'] > 50000:\n",
    "        print(f\"     ‚Ä¢ Cluster VOLUMOSO ({int(row['total_empresas']):,} empresas)\")\n",
    "        print(f\"     ‚Ä¢ Foco: Automa√ß√£o e escala\")\n",
    "        print(f\"     ‚Ä¢ Abordagem: Comunica√ß√£o em massa, self-service\")\n",
    "        print(f\"     ‚Ä¢ KPI: Volume processado por colaborador\")\n",
    "    elif row['dias_medio'] > 500:\n",
    "        print(f\"     ‚Ä¢ Cluster ANTIGO (m√©dia {row['dias_medio']:.0f} dias)\")\n",
    "        print(f\"     ‚Ä¢ Foco: Recupera√ß√£o e a√ß√µes jur√≠dicas\")\n",
    "        print(f\"     ‚Ä¢ Abordagem: Avalia√ß√£o de viabilidade, execu√ß√£o fiscal\")\n",
    "        print(f\"     ‚Ä¢ KPI: Taxa de recupera√ß√£o via judicial\")\n",
    "    else:\n",
    "        print(f\"     ‚Ä¢ Cluster BALANCEADO\")\n",
    "        print(f\"     ‚Ä¢ Foco: Processo padr√£o de cobran√ßa\")\n",
    "        print(f\"     ‚Ä¢ Abordagem: Workflow regular, follow-up sistem√°tico\")\n",
    "        print(f\"     ‚Ä¢ KPI: Taxa de convers√£o em pagamento\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ AN√ÅLISES ESTRAT√âGICAS CONCLU√çDAS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTabelas criadas/atualizadas:\")\n",
    "print(\"  ‚Ä¢ gecob.prior_clusters_empresas\")\n",
    "print(\"  ‚Ä¢ gecob.prior_outliers_identificados\")\n",
    "print(\"  ‚Ä¢ gecob.prior_acoes_prioritarias (TOP 50 para a√ß√£o)\")\n",
    "print(\"\\nPr√≥ximos passos:\")\n",
    "print(\"  1. Exportar listas priorit√°rias para equipes de cobran√ßa\")\n",
    "print(\"  2. Configurar dashboards de monitoramento\")\n",
    "print(\"  3. Estabelecer rotina de atualiza√ß√£o semanal\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# PARTE 6: VISUALIZA√á√ïES AVAN√áADAS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"6. GERANDO VISUALIZA√á√ïES AVAN√áADAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dashboard Executivo\n",
    "print(\"\\nGerando Dashboard Executivo...\")\n",
    "\n",
    "fig = plt.figure(figsize=(24, 16))\n",
    "gs = fig.add_gridspec(4, 4, hspace=0.35, wspace=0.35)\n",
    "\n",
    "fig.suptitle('DASHBOARD EXECUTIVO - SISTEMA DE PRIORIZA√á√ÉO GECOB/SC', \n",
    "             fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "# Preparar dados agregados\n",
    "df_stats_agg = df_stats_clusters.copy()\n",
    "\n",
    "# 1. Distribui√ß√£o de Valor por Cluster (Pizza)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "valores_cluster = df_stats_agg.groupby('cluster')['valor_total_cluster'].sum()\n",
    "colors_pie = plt.cm.Set3(np.linspace(0, 1, len(valores_cluster)))\n",
    "wedges, texts, autotexts = ax1.pie(valores_cluster/1e9, \n",
    "                                     labels=[f\"C{int(c)}\" for c in valores_cluster.index], \n",
    "                                     autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "    autotext.set_fontsize(10)\n",
    "    autotext.set_fontweight('bold')\n",
    "ax1.set_title('Distribui√ß√£o de Valor\\npor Cluster (Bilh√µes)', fontweight='bold', fontsize=12)\n",
    "\n",
    "# 2. Empresas por Cluster (Barras)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "empresas_cluster = df_stats_agg.groupby('cluster')['qtd_empresas'].sum()\n",
    "bars = ax2.bar([f\"C{int(c)}\" for c in empresas_cluster.index], empresas_cluster.values/1000, \n",
    "               color=colors_pie)\n",
    "ax2.set_xlabel('Cluster', fontweight='bold')\n",
    "ax2.set_ylabel('Empresas (Milhares)', fontweight='bold')\n",
    "ax2.set_title('Volume de Empresas', fontweight='bold', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(empresas_cluster.values/1000):\n",
    "    ax2.text(i, v, f'{v:.0f}k', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 3. Curva ABC\n",
    "ax3 = fig.add_subplot(gs[0, 2:])\n",
    "ax3.plot(df_abc['cluster'].astype(str), df_abc['pct_acumulado'], \n",
    "         marker='o', linewidth=3, markersize=10, color='darkblue', label='% Acumulado')\n",
    "ax3.axhline(y=80, color='red', linestyle='--', linewidth=2, label='80% (Pareto)')\n",
    "ax3.axhline(y=95, color='orange', linestyle='--', linewidth=2, label='95%')\n",
    "ax3.fill_between(range(len(df_abc)), 0, df_abc['pct_acumulado'], alpha=0.2)\n",
    "ax3.set_xlabel('Cluster', fontweight='bold')\n",
    "ax3.set_ylabel('% Acumulado do Valor', fontweight='bold')\n",
    "ax3.set_title('Curva ABC - Concentra√ß√£o de Valor', fontweight='bold', fontsize=12)\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(0, 105)\n",
    "\n",
    "# 4-5. Distribui√ß√£o por Faixas (se houver dados)\n",
    "if len(df_faixas) > 0:\n",
    "    ax4 = fig.add_subplot(gs[1, :2])\n",
    "    pivot_faixas = df_faixas.pivot_table(index='cluster', columns='faixa_valor', \n",
    "                                          values='empresas', fill_value=0)\n",
    "    pivot_faixas.plot(kind='bar', stacked=True, ax=ax4, \n",
    "                      colormap='RdYlGn_r', width=0.7)\n",
    "    ax4.set_xlabel('Cluster', fontweight='bold')\n",
    "    ax4.set_ylabel('N√∫mero de Empresas', fontweight='bold')\n",
    "    ax4.set_title('Distribui√ß√£o por Faixa de Valor', fontweight='bold', fontsize=12)\n",
    "    ax4.legend(title='Faixa', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "# 6. Aging\n",
    "if len(df_aging) > 0:\n",
    "    ax5 = fig.add_subplot(gs[1, 2:])\n",
    "    pivot_aging = df_aging.pivot_table(index='cluster', columns='faixa_aging', \n",
    "                                        values='valor_total', fill_value=0) / 1e6\n",
    "    pivot_aging.plot(kind='bar', stacked=True, ax=ax5, \n",
    "                     colormap='YlOrRd', width=0.7)\n",
    "    ax5.set_xlabel('Cluster', fontweight='bold')\n",
    "    ax5.set_ylabel('Valor (Milh√µes R$)', fontweight='bold')\n",
    "    ax5.set_title('Aging de D√©bitos', fontweight='bold', fontsize=12)\n",
    "    ax5.legend(title='Aging', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax5.xaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "# 7. Matriz de Correla√ß√£o\n",
    "ax6 = fig.add_subplot(gs[2, :2])\n",
    "corr_data = df_correlacao[['corr_valor_dias', 'corr_valor_score', 'corr_dias_score']].T\n",
    "im = ax6.imshow(corr_data, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "ax6.set_xticks(range(len(df_correlacao)))\n",
    "ax6.set_xticklabels([f\"C{int(c)}\" for c in df_correlacao['cluster']])\n",
    "ax6.set_yticks(range(3))\n",
    "ax6.set_yticklabels(['Valor√óDias', 'Valor√óScore', 'Dias√óScore'], fontsize=10)\n",
    "ax6.set_title('Correla√ß√µes por Cluster', fontweight='bold', fontsize=12)\n",
    "cbar = plt.colorbar(im, ax=ax6)\n",
    "cbar.set_label('Correla√ß√£o', fontweight='bold')\n",
    "\n",
    "# Adicionar valores na matriz\n",
    "for i in range(3):\n",
    "    for j in range(len(df_correlacao)):\n",
    "        val = corr_data.iloc[i, j]\n",
    "        color = 'white' if builtins.abs(val) > 0.5 else 'black'\n",
    "        ax6.text(j, i, f'{val:.2f}', ha=\"center\", va=\"center\", \n",
    "                color=color, fontsize=10, fontweight='bold')\n",
    "\n",
    "# 8-9. Outliers\n",
    "if len(df_stats_outliers) > 0:\n",
    "    ax7 = fig.add_subplot(gs[2, 2])\n",
    "    colors_out = ['#ffdd70', '#ff7f0e', '#d62728'][:len(df_stats_outliers)]\n",
    "    ax7.bar([f\"Nv{int(s)}\" for s in df_stats_outliers['nivel_severidade']], \n",
    "            df_stats_outliers['empresas'],\n",
    "            color=colors_out)\n",
    "    ax7.set_xlabel('Severidade', fontweight='bold')\n",
    "    ax7.set_ylabel('Empresas', fontweight='bold')\n",
    "    ax7.set_title('Outliers por Severidade', fontweight='bold', fontsize=12)\n",
    "    ax7.grid(True, alpha=0.3, axis='y')\n",
    "    for i, v in enumerate(df_stats_outliers['empresas']):\n",
    "        ax7.text(i, v, f'{int(v):,}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax8 = fig.add_subplot(gs[2, 3])\n",
    "    ax8.barh([f\"Nv{int(s)}\" for s in df_stats_outliers['nivel_severidade']], \n",
    "             df_stats_outliers['valor_total']/1e9,\n",
    "             color=colors_out)\n",
    "    ax8.set_xlabel('Valor (Bilh√µes R$)', fontweight='bold')\n",
    "    ax8.set_title('Valor em Outliers', fontweight='bold', fontsize=12)\n",
    "    ax8.invert_yaxis()\n",
    "    for i, v in enumerate(df_stats_outliers['valor_total']/1e9):\n",
    "        ax8.text(v, i, f' R$ {v:.2f}B', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 10. Plano de A√ß√£o\n",
    "ax9 = fig.add_subplot(gs[3, :2])\n",
    "x = np.arange(len(df_plano))\n",
    "width = 0.2\n",
    "bars1 = ax9.bar(x - 1.5*width, df_plano['criticos'], width, label='üî¥ Cr√≠ticos', color='#d62728')\n",
    "bars2 = ax9.bar(x - 0.5*width, df_plano['altos'], width, label='üü† Altos', color='#ff7f0e')\n",
    "bars3 = ax9.bar(x + 0.5*width, df_plano['medios'], width, label='üü° M√©dios', color='#ffdd70')\n",
    "bars4 = ax9.bar(x + 1.5*width, df_plano['baixos'], width, label='üü¢ Baixos', color='#2ca02c')\n",
    "ax9.set_xlabel('Cluster', fontweight='bold')\n",
    "ax9.set_ylabel('N√∫mero de Empresas', fontweight='bold')\n",
    "ax9.set_title('Estratifica√ß√£o por Prioridade', fontweight='bold', fontsize=12)\n",
    "ax9.set_xticks(x)\n",
    "ax9.set_xticklabels([f\"C{int(c)}\" for c in df_plano['cluster']])\n",
    "ax9.legend(fontsize=9)\n",
    "ax9.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 11. Capacidade vs Demanda\n",
    "ax10 = fig.add_subplot(gs[3, 2:])\n",
    "ax10_twin = ax10.twinx()\n",
    "ax10.bar([f\"C{int(c)}\" for c in df_plano['cluster']], df_plano['total_empresas']/1000, \n",
    "         alpha=0.6, color='steelblue', label='Total Empresas (mil)')\n",
    "ax10_twin.plot([f\"C{int(c)}\" for c in df_plano['cluster']], df_plano['capacidade_mensal_estimada']/1000,\n",
    "               marker='o', color='red', linewidth=3, markersize=10, label='Capacidade/M√™s (mil)')\n",
    "ax10.set_xlabel('Cluster', fontweight='bold')\n",
    "ax10.set_ylabel('Total Empresas (Milhares)', color='steelblue', fontweight='bold')\n",
    "ax10_twin.set_ylabel('Capacidade Mensal (Milhares)', color='red', fontweight='bold')\n",
    "ax10.set_title('Dimensionamento: Demanda vs Capacidade', fontweight='bold', fontsize=12)\n",
    "ax10.tick_params(axis='y', labelcolor='steelblue')\n",
    "ax10_twin.tick_params(axis='y', labelcolor='red')\n",
    "ax10.grid(True, alpha=0.3, axis='y')\n",
    "ax10.legend(loc='upper left', fontsize=9)\n",
    "ax10_twin.legend(loc='upper right', fontsize=9)\n",
    "\n",
    "plt.savefig('dashboard_priorizacao_gecob.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úì Dashboard salvo: dashboard_priorizacao_gecob.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ AN√ÅLISES SQL COMPLETAS FINALIZADAS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nArquivos gerados:\")\n",
    "print(\"  ‚Ä¢ dashboard_priorizacao_gecob.png - Dashboard executivo\")\n",
    "print(\"\\nTabelas atualizadas:\")\n",
    "print(\"  ‚Ä¢ gecob.prior_clusters_empresas\")\n",
    "print(\"  ‚Ä¢ gecob.prior_outliers_identificados\")\n",
    "print(\"\\nPr√≥ximos passos sugeridos:\")\n",
    "print(\"  1. Exportar Top 50 prioridades para a√ß√£o imediata\")\n",
    "print(\"  2. Criar views para monitoramento cont√≠nuo\")\n",
    "print(\"  3. Automatizar relat√≥rios semanais\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Pipeline)",
   "language": "python",
   "name": "conda_data_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
